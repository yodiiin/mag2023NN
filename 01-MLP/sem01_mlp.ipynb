{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2498b833-4df0-4ac6-93ef-65f9126b2ac8",
   "metadata": {},
   "source": [
    "Давайте попробуем написать многослойный перцептрон с нуля, пользуясь знаниями, полученными на лекции: реализуем его в numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dadbbf8-1eb8-4b11-b243-067ad0ff8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62026dbf-c94d-43bc-81e0-5222b15d8a17",
   "metadata": {},
   "source": [
    "Мы с вами знаем, что основная часть нейронной сети - это нейрон, или линейная регрессия, пропущенная через функцию активации. Давайте напишем класс для нашего будущего нейрона, а заодно и функцию активации, через которую будет проходить сигнал:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5c27fb-c100-4e29-a904-164181667622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    # Наша функция активации: f(x) = 1 / (1 + e^(-x))\n",
    "    # your code here\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    # сетка получает х на вход (х - инпут)\n",
    "    def feedforward(self, inputs):\n",
    "        # Умножаем входы на веса, прибавляем порог, затем используем функцию активации\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))    # 0.9990889488055994"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c5fcf-4702-4e8e-86cb-64c8af452498",
   "metadata": {},
   "source": [
    "Чтобы создать полноценную нейронную сеть, нужно объединить нейроны в слой. Давайте реализуем следующую архитектуру:\n",
    "\n",
    "<img src=\"https://media.proglib.io/posts/2020/10/02/de81e6549b3e3c3bc1e3fdc78fe59f9c.png\" />\n",
    "\n",
    "У этой сети два входа, скрытый слой с двумя нейронами ($h_1$ и $h_2$) и выходной слой с одним нейроном ($o_1$). Обратите внимание, что входы для $o_1$ – это выходы из $h_1$ и $h_2$. Именно это создает из нейронов сеть.\n",
    "\n",
    "Давайте реализуем прямую связь для нашей нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8605151e-fd71-4a07-8858-5a39fe564516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    '''\n",
    "    Нейронная сеть с:\n",
    "    - 2 входами\n",
    "    - скрытым слоем с 2 нейронами (h1, h2)\n",
    "    - выходным слоем с 1 нейроном (o1)\n",
    "    Все нейроны имеют одинаковые веса и пороги:\n",
    "    - w = [0, 1]\n",
    "    - b = 0\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        weights = np.array([0, 1])\n",
    "        bias = 0\n",
    "\n",
    "        # Используем класс Neuron, написанный выше\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "        # Входы для o1 - это выходы h1 и h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "        return out_o1\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x)) # 0.7216325609518421"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff2ffe-8262-471f-9e62-b89d175fc373",
   "metadata": {},
   "source": [
    "Допустим, у нас есть следующие измерения:\n",
    "\n",
    "| Имя | Вес (в фунтах) | Рост (в дюймах) | Пол |\n",
    "|---|---|---|---|\n",
    "| Алиса | 133 (54.4 кг) | 65 (165,1 см) | Ж |\n",
    "| Боб | 160 (65,44 кг) | 72 (183 см) | М |\n",
    "| Чарли | 152 (62.2 кг) | 70 (178 см) | М |\n",
    "| Диана | 120 (49 кг) | 60 (152 см) | Ж |\n",
    "\n",
    "\n",
    "Давайте обучим нашу нейронную сеть предсказывать пол человека по его росту и весу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b6364-3d1f-4069-a237-ac1c820dc1f4",
   "metadata": {},
   "source": [
    "Мы будем представлять мужской пол как 0, женский – как 1, а также сдвинем данные, чтобы их было проще использовать:\n",
    "\n",
    "| Имя | Вес - 135 | Рост - 66 | Пол |\n",
    "|---|---|---|---|\n",
    "| Алиса | -2 | -1 | 1 |\n",
    "| Боб | 25 | 6 | 0 |\n",
    "| Чарли | 17 | 4 | 0 |\n",
    "| Диана | -15 | -6 | 1 |\n",
    "\n",
    "Сделаем сдвиг, чтобы было проще считать (обычно сдвигают на среднее арифметическое, но здесь циферки просто подобраны, чтобы потом было легче). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d3484-e7d7-46e2-a9cb-e62bc848bc60",
   "metadata": {},
   "source": [
    "Прежде чем обучать нашу нейронную сеть, нам нужно как-то измерить, насколько \"хорошо\" она работает, чтобы она смогла работать \"лучше\". Это измерение и есть потери (loss).\n",
    "\n",
    "Мы используем для расчета потерь среднюю квадратичную ошибку (mean squared error, MSE):\n",
    "\n",
    "$$MSE = \\frac{1} {n} \\sum_{i=1}^{n} (y_{true} - y_{pred})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68167f30",
   "metadata": {},
   "source": [
    "y_true — число\n",
    "\n",
    "y_pred — функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a4e33a-b164-494c-b768-f26ff8c499d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true и y_pred - массивы numpy одинаковой длины\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "\n",
    "print(mse_loss(y_true, y_pred)) # 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d259e-b312-4466-9c23-25b15bd402d9",
   "metadata": {},
   "source": [
    "Теперь у нас есть четкая цель: минимизировать потери нейронной сети. Мы знаем, что можем изменять веса и пороги нейронов, чтобы изменить ее предсказания, но как нам делать это таким образом, чтобы минимизировать потери?\n",
    "\n",
    "Будем рассматривать функцию потерь как функцию от весов и порогов. Давайте отметим все веса и пороги нашей нейронной сети:\n",
    "\n",
    "<img src=\"https://media.proglib.io/posts/2020/10/03/b3ae8de6555967d30fd4092654358e18.png\" />\n",
    "\n",
    "Теперь мы можем записать функцию потерь как функцию от нескольких переменных:\n",
    "\n",
    "$$L(w_1, w_2, w_3, w_4, w_5, w_6, b_1, b_2, b_3)$$\n",
    "\n",
    "Предположим, мы хотим отрегулировать $w_1$. Как изменится значение потери L при изменении $w_1$? На этот вопрос может ответить частная производная $\\frac {\\partial L} {\\partial w_1}$. Как мы ее рассчитаем?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e07c42-ba20-4d0f-b03a-53ee6b8a2116",
   "metadata": {},
   "source": [
    "Прежде всего, давайте перепишем эту частную производную через $\\frac {\\partial y_{pred}} {\\partial w_1}$, воспользовавшись цепным правилом. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5498ba-7d64-492c-abff-05f4ede54b27",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L} {\\partial w_1} = \\frac{\\partial L} {\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e7291-4a68-4770-9d26-25f936344dcc",
   "metadata": {},
   "source": [
    "Мы можем рассчитать $\\frac{\\partial L}{\\partial y_{pred}}$, поскольку $y_{true} = 1$, а значит, $L = (1 - y_{pred})^2$:\n",
    "\n",
    "$\\frac{\\partial L} {\\partial y_{pred}} = \\frac{\\partial (1 - y_{pred})^2} {\\partial y_{pred}} = -2(1 - y_{pred})$\n",
    "\n",
    "Теперь давайте решим, что делать с $\\frac{\\partial y_{pred}}{\\partial w_1}$. Обозначая выходы нейронов, как прежде, $h_1$, $h_2$ и $o_1$, получаем:\n",
    "\n",
    "$y_{pred} = o_1 = f(w_5 h_1 + w_6 h_2 + b_3)$\n",
    "\n",
    "Вспомните, что f() – это наша функция активации, сигмоида. Поскольку $w_1$ влияет только на $h_1$ (но не на $h_2$), мы можем снова использовать цепное правило и записать:\n",
    "\n",
    "$\\frac{\\partial y_{pred}}{\\partial w_1} = \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}$\n",
    "\n",
    "$\\frac{\\partial y_{pred}}{\\partial h_1} = w_5 * f'(w_5 h_1 + w_6 h_2 + b_3)$\n",
    " \n",
    "Мы можем сделать то же самое для $\\frac{\\partial h_1}{\\partial w_1}$, снова применяя цепное правило:\n",
    "\n",
    "$h_1 = f(w_1 x_1 + w_2 x_2 + b_1)$\n",
    "\n",
    "$\\frac{\\partial h_1}{\\partial w_1} = x_1 * f'(w_1 x_1 + w_2 x_2 + b_1)$\n",
    " \n",
    " \n",
    "В этой формуле $x_1$ – это вес, а $x_2$ – рост. Вот уже второй раз мы встречаем $f'(x)$ – производную сигмоидной функции! Давайте вычислим ее:\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$f`(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = f(x) * (1 - f(x))$\n",
    " \n",
    " \n",
    "Мы используем эту красивую форму для $f'(x)$ позже. На этом мы закончили! Мы сумели разложить $\\frac {\\partial L} {\\partial w_1}$ на несколько частей, которые мы можем рассчитать:\n",
    "\n",
    "$\\frac{\\partial L} {\\partial w_1} = \\frac{\\partial L} {\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}$\n",
    "\n",
    "Такой метод расчета частных производных \"от конца к началу\" называется методом обратного распространения (backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b8a99-2188-4a68-923e-25c70d701a84",
   "metadata": {},
   "source": [
    "Будем считать, что наш набор данных пока состоит из одной Алисы (стохастический градиентный спуск!):\n",
    "\n",
    "| Имя | Вес - 141 | Рост - 67 | Пол |\n",
    "|---|---|---|---|\n",
    "| Алиса | -2 | -1 | 1 |\n",
    "\n",
    "Давайте инициализируем все веса как 1, а все пороги как 0. Если мы выполним прямой проход по нейронной сети, то получим:\n",
    "\n",
    "$h_1 = f(w_1 x_1 + w_2 x_2 + b_1) = f(-2 - 1 + 0) = 0.0474$\n",
    "$h_2 = f(w_3 x_1 + w_4 x_2 + b_2) = 0.0474$\n",
    "$o_1 = f(w_5 h_1 + w_6 h_2 + b_3) = f(0.0474 + 0.0474 + 0) = 0.524$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d90d6-63ea-4fb2-9854-962304ec7d2e",
   "metadata": {},
   "source": [
    "Наша сеть выдает $y_{pred} = 0.524$, что находится примерно на полпути между мужским полом (0) и женским (1). Давайте рассчитаем $\\frac{\\partial L} {\\partial w_1}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3717d6-2dbc-4ad4-b417-b7a9f1bcf1bb",
   "metadata": {},
   "source": [
    "    # посчитайте это самостоятельно!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f857a-be25-44d4-a210-1dc2d6482333",
   "metadata": {},
   "source": [
    "Должно получиться 0.0214.\n",
    "\n",
    "Результат говорит нам, что при увеличении $w_1$ функция ошибки чуть-чуть повышается. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c2922-5713-4685-beae-556abd95bdf8",
   "metadata": {},
   "source": [
    "Давайте используем алгоритм оптимизации под названием стохастический градиентный спуск (stochastic gradient descent), который определит, как мы будем изменять наши веса и пороги для минимизации потерь. Фактически, он заключается в следующей формуле обновления:\n",
    "\n",
    "$w_1 \\leftarrow w_1 - \\eta \\frac{\\partial L} {\\partial w_1}$\n",
    "\n",
    "$\\eta$ - гиперпараметр, называемый скоростью обучения. \n",
    "\n",
    "Итого, все, что нам нужно теперь делать - вычитать по описанной выше формуле. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d31b7-00ef-427b-8ee3-bfd233910595",
   "metadata": {},
   "source": [
    "Процесс обучения сети будет выглядеть примерно так:\n",
    "\n",
    "1. Выбираем одно наблюдение из набора данных. Именно то, что мы работаем только с одним наблюдением, делает наш градиентный спуск стохастическим.\n",
    "2. Считаем все частные производные функции потерь по всем весам и порогам ($\\frac{\\partial L} {\\partial w_1}$, $\\frac{\\partial L} {\\partial w_2}$ и т.д.)\n",
    "3. Используем формулу обновления, чтобы обновить значения каждого веса и порога.\n",
    "4. Снова переходим к шагу 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e066ffc-65c6-48f7-9c3e-c068a09f446a",
   "metadata": {},
   "source": [
    "Итак, соберем финальный код для всей нашей сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247ba4da-b0e1-4493-ae55-32467fd430ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.401\n",
      "Epoch 10 loss: 0.225\n",
      "Epoch 20 loss: 0.181\n",
      "Epoch 30 loss: 0.144\n",
      "Epoch 40 loss: 0.116\n",
      "Epoch 50 loss: 0.094\n",
      "Epoch 60 loss: 0.077\n",
      "Epoch 70 loss: 0.065\n",
      "Epoch 80 loss: 0.055\n",
      "Epoch 90 loss: 0.047\n",
      "Epoch 100 loss: 0.041\n",
      "Epoch 110 loss: 0.037\n",
      "Epoch 120 loss: 0.033\n",
      "Epoch 130 loss: 0.029\n",
      "Epoch 140 loss: 0.027\n",
      "Epoch 150 loss: 0.024\n",
      "Epoch 160 loss: 0.022\n",
      "Epoch 170 loss: 0.021\n",
      "Epoch 180 loss: 0.019\n",
      "Epoch 190 loss: 0.018\n",
      "Epoch 200 loss: 0.017\n",
      "Epoch 210 loss: 0.016\n",
      "Epoch 220 loss: 0.015\n",
      "Epoch 230 loss: 0.014\n",
      "Epoch 240 loss: 0.013\n",
      "Epoch 250 loss: 0.013\n",
      "Epoch 260 loss: 0.012\n",
      "Epoch 270 loss: 0.011\n",
      "Epoch 280 loss: 0.011\n",
      "Epoch 290 loss: 0.010\n",
      "Epoch 300 loss: 0.010\n",
      "Epoch 310 loss: 0.010\n",
      "Epoch 320 loss: 0.009\n",
      "Epoch 330 loss: 0.009\n",
      "Epoch 340 loss: 0.008\n",
      "Epoch 350 loss: 0.008\n",
      "Epoch 360 loss: 0.008\n",
      "Epoch 370 loss: 0.008\n",
      "Epoch 380 loss: 0.007\n",
      "Epoch 390 loss: 0.007\n",
      "Epoch 400 loss: 0.007\n",
      "Epoch 410 loss: 0.007\n",
      "Epoch 420 loss: 0.007\n",
      "Epoch 430 loss: 0.006\n",
      "Epoch 440 loss: 0.006\n",
      "Epoch 450 loss: 0.006\n",
      "Epoch 460 loss: 0.006\n",
      "Epoch 470 loss: 0.006\n",
      "Epoch 480 loss: 0.006\n",
      "Epoch 490 loss: 0.005\n",
      "Epoch 500 loss: 0.005\n",
      "Epoch 510 loss: 0.005\n",
      "Epoch 520 loss: 0.005\n",
      "Epoch 530 loss: 0.005\n",
      "Epoch 540 loss: 0.005\n",
      "Epoch 550 loss: 0.005\n",
      "Epoch 560 loss: 0.005\n",
      "Epoch 570 loss: 0.005\n",
      "Epoch 580 loss: 0.004\n",
      "Epoch 590 loss: 0.004\n",
      "Epoch 600 loss: 0.004\n",
      "Epoch 610 loss: 0.004\n",
      "Epoch 620 loss: 0.004\n",
      "Epoch 630 loss: 0.004\n",
      "Epoch 640 loss: 0.004\n",
      "Epoch 650 loss: 0.004\n",
      "Epoch 660 loss: 0.004\n",
      "Epoch 670 loss: 0.004\n",
      "Epoch 680 loss: 0.004\n",
      "Epoch 690 loss: 0.004\n",
      "Epoch 700 loss: 0.004\n",
      "Epoch 710 loss: 0.004\n",
      "Epoch 720 loss: 0.003\n",
      "Epoch 730 loss: 0.003\n",
      "Epoch 740 loss: 0.003\n",
      "Epoch 750 loss: 0.003\n",
      "Epoch 760 loss: 0.003\n",
      "Epoch 770 loss: 0.003\n",
      "Epoch 780 loss: 0.003\n",
      "Epoch 790 loss: 0.003\n",
      "Epoch 800 loss: 0.003\n",
      "Epoch 810 loss: 0.003\n",
      "Epoch 820 loss: 0.003\n",
      "Epoch 830 loss: 0.003\n",
      "Epoch 840 loss: 0.003\n",
      "Epoch 850 loss: 0.003\n",
      "Epoch 860 loss: 0.003\n",
      "Epoch 870 loss: 0.003\n",
      "Epoch 880 loss: 0.003\n",
      "Epoch 890 loss: 0.003\n",
      "Epoch 900 loss: 0.003\n",
      "Epoch 910 loss: 0.003\n",
      "Epoch 920 loss: 0.003\n",
      "Epoch 930 loss: 0.003\n",
      "Epoch 940 loss: 0.003\n",
      "Epoch 950 loss: 0.003\n",
      "Epoch 960 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    # Наша функция активации: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    # Производная сигмоиды: f'(x) = f(x) * (1 - f(x))\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true и y_pred - массивы numpy одинаковой длины\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    '''\n",
    "    Нейронная сеть с:\n",
    "    - 2 входами\n",
    "    - скрытым слоем с 2 нейронами (h1, h2)\n",
    "    - выходной слой с 1 нейроном (o1)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Веса\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        # Пороги\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # x - массив numpy с двумя элементами\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3) # если число логит то надо ещё его пропустить через сигмоиду или софт макс чтобы получить вероятность\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        '''\n",
    "        - data - массив numpy (n x 2) numpy, n = к-во наблюдений в наборе. \n",
    "        - all_y_trues - массив numpy с n элементами.\n",
    "          Элементы all_y_trues соответствуют наблюдениям в data.\n",
    "        '''\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000 # сколько раз пройти по всему набору данных \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # --- Прямой проход (эти значения нам понадобятся позже)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "\n",
    "                # --- Считаем частные производные.\n",
    "                # --- Имена: d_L_d_w1 = \"частная производная L по w1\"\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Нейрон o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Нейрон h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # Нейрон h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "                # --- Обновляем веса и пороги\n",
    "                # Нейрон h1\n",
    "                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Нейрон h2\n",
    "                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "                # Нейрон o1\n",
    "                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "              # --- Считаем полные потери в конце каждой эпохи\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "\n",
    "# Определим набор данных\n",
    "data = np.array([\n",
    "  [-2, -1],  # Алиса\n",
    "  [25, 6],   # Боб\n",
    "  [17, 4],   # Чарли\n",
    "  [-15, -6], # Диана\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Алиса\n",
    "  0, # Боб\n",
    "  0, # Чарли\n",
    "  1, # Диана\n",
    "])\n",
    "\n",
    "# Обучаем нашу нейронную сеть!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c68799-1404-44cf-af68-ad4e6dbbbc55",
   "metadata": {},
   "source": [
    "Потестируем на ранее не виденных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44993fb9-739c-4718-ba17-f828d564bdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эмили: 0.949\n",
      "Фрэнк: 0.040\n"
     ]
    }
   ],
   "source": [
    "emily = np.array([-7, -3]) # 128 фунтов (52.35 кг), 63 дюйма (160 см)\n",
    "frank = np.array([20, 2])  # 155 фунтов (63.4 кг), 68 дюймов (173 см)\n",
    "print(f\"Эмили: {network.feedforward(emily):.3f}\")\n",
    "print(f\"Фрэнк: {network.feedforward(frank):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
