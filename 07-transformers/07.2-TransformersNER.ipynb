{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/rsuh-python/mag2023NN/refs/heads/main/07-transformers/train.txt\n",
    "!wget https://raw.githubusercontent.com/rsuh-python/mag2023NN/refs/heads/main/07-transformers/val.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наш кастомный датасет - это просто текстоый файл, где каждое предложение отделено от другого пустой строкой, а лейблы записаны через таб к токенам. Нам нужно сперва распарсить этот датасет, потом написать класс для него и наконец собрать все в dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(filepath):\n",
    "    sentences, labels = [], []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        current_sentence, current_labels = [], []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # конец предложения\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    labels.append(current_labels)\n",
    "                current_sentence, current_labels = [], []\n",
    "            else:\n",
    "                token, tag = line.split('\\t')\n",
    "                current_sentence.append(token)\n",
    "                current_labels.append(tag)\n",
    "        # добавим последнее предложение на случай, если файл не заканчивался на пустую строчку\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            labels.append(current_labels)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, label2id, max_length=128):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.sentences[idx]\n",
    "        tags = self.labels[idx]\n",
    "\n",
    "        # токенизируем\n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # свяжем лейблы с токенизированным аутпутом: у нас subwords\n",
    "        labels = []\n",
    "        word_ids = encoding.word_ids()  # мапим ID токенов с ID слов\n",
    "        previous_word_id = None\n",
    "\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:  # Special tokens or padding\n",
    "                labels.append(-100)\n",
    "            elif word_id != previous_word_id:  # начало нового слова\n",
    "                labels.append(self.label2id.get(tags[word_id], -100))\n",
    "            else:  # Subword\n",
    "                labels.append(-100)\n",
    "            previous_word_id = word_id\n",
    "\n",
    "        encoding[\"labels\"] = labels\n",
    "        encoding.pop(\"offset_mapping\")  # для модели не нужно\n",
    "\n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(path, tokenizer, label2id, batch_size=16, max_length=128):\n",
    "    sentences, labels = parse_dataset(path)\n",
    "    dataset = NERDataset(sentences, labels, tokenizer, label2id, max_length=max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим теги и их ID\n",
    "labels = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# загрузим токенизатор и модель\n",
    "model_checkpoint = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# соберем лоудеры\n",
    "batch_size = 32\n",
    "train_dataloader = create_dataloader('train.txt', tokenizer, label2id, batch_size=batch_size)\n",
    "val_dataloader = create_dataloader('val.txt', tokenizer, label2id, batch_size=batch_size)\n",
    "\n",
    "# проверим, что все ок\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что у нас в батче"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['input_ids'][0], batch['labels'][0])\n",
    "tokenizer.decode(batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# приготовим модель к обучению\n",
    "# transformers должен предупредить, что нам надо бы потренировать модельку - значит, все ок.\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем использовать стандартную метрику seqeval из модуля evaluate\n",
    "metric = evaluate.load('seqeval')\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "\n",
    "    # логиты в индексы\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    # союда будем собирать необходимое барахло\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        # Align predictions and labels, ignoring special tokens (-100)\n",
    "        current_predictions = []\n",
    "        current_labels = []\n",
    "\n",
    "        for pred, lab in zip(prediction, label):\n",
    "            if lab != -100:  # игнорим паддинг\n",
    "                current_predictions.append(id2label[pred])\n",
    "                current_labels.append(id2label[lab])\n",
    "\n",
    "        # добавляем в главные два листа\n",
    "        true_predictions.append(current_predictions)\n",
    "        true_labels.append(current_labels)\n",
    "\n",
    "    # пихнем в метрику и получим результат\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В трансформерах все петли написаны за нас: нам остается передать аргументы для обучения (их много) и собственно запустить трейнер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    eval_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    learning_rate=5e-5,             # Learning rate\n",
    "    per_device_train_batch_size=32, # Batch size for training\n",
    "    per_device_eval_batch_size=32,  # Batch size for evaluation\n",
    "    num_train_epochs=3,             # Number of epochs\n",
    "    weight_decay=0.01,              # Strength of weight decay\n",
    "    logging_dir=\"./logs\",           # Directory for storing logs\n",
    "    logging_steps=10,               # Log every 10 steps\n",
    "    save_strategy=\"epoch\",          # Save model after each epoch\n",
    "    load_best_model_at_end=True,    # Load the best model after training\n",
    "    metric_for_best_model=\"f1\",     # Use F1 score to choose the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализировали\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataloader.dataset,  # Training dataset\n",
    "    eval_dataset=val_dataloader.dataset,   # Evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собственно обучение - автоматически делает логи\n",
    "trainer.train()\n",
    "\n",
    "# оценим модельку\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "# Сохраним, что получилось\n",
    "trainer.save_model(\"./ner_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем, так сказать, качественную оценку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, tokenizer, model):\n",
    "    # токенизируем исходное предложение\n",
    "    tokens = tokenizer(\n",
    "        sentence.split(),\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    word_ids = tokens.word_ids()  # мапим токены по индексам слов\n",
    "    with torch.no_grad():\n",
    "        tokens.to('cuda')\n",
    "        outputs = model(**tokens)\n",
    "        predictions = outputs.logits.argmax(dim=-1).squeeze().tolist()\n",
    "\n",
    "    # будем элайнить лейблы по словам\n",
    "    aligned_predictions = []\n",
    "    current_word_id = None\n",
    "\n",
    "    for word_id, prediction in zip(word_ids, predictions):\n",
    "        if word_id is not None and word_id != current_word_id:  # начало нового слова\n",
    "            aligned_predictions.append(id2label[prediction])\n",
    "            current_word_id = word_id\n",
    "\n",
    "    # зазипим результаты\n",
    "    result = list(zip(sentence.split(), aligned_predictions))\n",
    "    return result\n",
    "\n",
    "\n",
    "# Проверка\n",
    "example_sentence = \"Eftir að hafa gegnt herskyldu í fyrri heimsstyrjöldinni hóf Hubble störf við stjörnuathugunarstöðina á Wilson - fjalli í Kaliforníu.\"\n",
    "print(predict(example_sentence, tokenizer, model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование готовых инструментов - хорошо, но иногда перед нами стоит задача модифицировать архитектуру модели, а то и вообще написать свою собственную с нуля, только используя эмбеддинги берта. Давайте перепишем архитектуру модели без использования автомодели трансформеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_of_classes):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "        self.classifier = nn.Linear(768, num_of_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
    "        # получим эмбеддинги токенов от берта\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids  \n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # собственно классификатор\n",
    "        logits = self.classifier(sequence_output)  # Shape: (batch_size, seq_len, num_classes)\n",
    "\n",
    "        # Чтобы использовать нашу модель с трейнером трансформеров, нам нужно тут же и лосс посчитать\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            # Flatten logits and labels for loss computation\n",
    "            logits_flat = logits.view(-1, logits.shape[-1])  # Shape: (batch_size * seq_len, num_classes)\n",
    "            labels_flat = labels.view(-1)  # Shape: (batch_size * seq_len)\n",
    "            loss = loss_fn(logits_flat, labels_flat)  # Scalar loss\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удостоверимся, что наша модель адекватно работает с датасетом. Батч возвращает нам словарь с ключами, который при распаковке как раз даст нам все то, что мы прописали в форварде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(len(labels))\n",
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем повторно трейнер, но уже с новой самописной моделькой\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataloader.dataset,  # Training dataset\n",
    "    eval_dataset=val_dataloader.dataset,   # Evaluation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "trainer.save_model(\"./ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и проверим точно так же, как предыдущую версию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Eftir að hafa gegnt herskyldu í fyrri heimsstyrjöldinni hóf Hubble störf við stjörnuathugunarstöðina á Wilson - fjalli í Kaliforníu.\"\n",
    "print(predict(example_sentence, tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание. \n",
    "\n",
    "В текущей версии задачи мы берем, по сути, только предсказание модели для первого подслова в слове: но что, если остальные подслова могли бы тоже влиять? Попробуйте доработать код таким образом, чтобы в обучающем датасете каждому подслову слова приписывался тег всего слова, а при предсказании модель выбирала тег слова более обдуманно: например, при трех и более подсловах голосованием. Поэкспериментируйте!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
