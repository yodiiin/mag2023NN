{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9EhWoZef-X8u",
   "metadata": {
    "collapsed": true,
    "id": "9EhWoZef-X8u"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Q9th7mpc-X8v",
   "metadata": {
    "executionInfo": {
     "elapsed": 2709,
     "status": "ok",
     "timestamp": 1680523105271,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "Q9th7mpc-X8v"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def print_encoding(model_inputs, indent=4):\n",
    "    indent_str = \" \" * indent\n",
    "    print(\"{\")\n",
    "    for k, v in model_inputs.items():\n",
    "        print(indent_str + k + \":\")\n",
    "        print(indent_str + indent_str + str(v))\n",
    "    print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qmXezUMg2idv",
   "metadata": {
    "id": "qmXezUMg2idv"
   },
   "source": [
    "## Часть 0: Стандартные практики Hugging Face Transformers\n",
    "\n",
    "Посмотрим на библиотеку Transformers на примере задачи sentiment analysis. \n",
    "\n",
    "Сперва найдем модель на [сайте](https://huggingface.co/models). Все желающие могут загружать туда свои обученные модели; наша модель описана в [этой статье](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3489963)).\n",
    "\n",
    "Потом нам нужно инициализировать два объекта: токенизатор и модель."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ySLmJ0Z-oD35",
   "metadata": {
    "id": "ySLmJ0Z-oD35"
   },
   "source": [
    "![full_nlp_pipeline.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mcsii_O42Z8Q",
   "metadata": {
    "id": "Mcsii_O42Z8Q"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Инициализируем токенизатор: возьмем предобученный и положим в пустой класс-контейнер\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "# То же с моделью. Модель берем для классификации последовательностей (задача many-to-one)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kT_zeWRBoD36",
   "metadata": {
    "id": "kT_zeWRBoD36"
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "inputs = \"I'm excited to learn about Hugging Face Transformers!\"\n",
    "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\") # возвращает id токенов словаря в тензорах, умеет в tf тоже\n",
    "outputs = model(**tokenized_inputs) # распаковка словаря\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "prediction = torch.argmax(outputs.logits)\n",
    "\n",
    "\n",
    "print(\"Input:\")\n",
    "print(inputs)\n",
    "print()\n",
    "print(\"Tokenized Inputs:\")\n",
    "print_encoding(tokenized_inputs)\n",
    "print()\n",
    "print(\"Model Outputs:\")\n",
    "print(outputs)\n",
    "print()\n",
    "print(f\"The prediction is {labels[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kXHB5wumCTk4",
   "metadata": {
    "id": "kXHB5wumCTk4"
   },
   "source": [
    "Attention mask показывает все слова, которые мы хотим учесть при расчете attention (мы не хотим включать в attention пады)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7jvH9haoD37",
   "metadata": {
    "id": "a7jvH9haoD37"
   },
   "source": [
    "### 0.1 Токенизаторы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43FLbwgz-X83",
   "metadata": {
    "id": "43FLbwgz-X83"
   },
   "source": [
    "Обычно предобученные модели имеют при себе и соответствующие токенизаторы, которые нужны для предобработки инпутов. Токенизаторы принимают сырые строки или списки строк и возвращают словари, которые содержат инпуты модели. \n",
    "\n",
    "Можно подключать токенизаторы либо так, как выше (с помощью пустого класса AutoTokenizer), либо как в коде ниже: с помощью специфического для модели класса (тут у нас DistilBERT). У многих моделей есть два токенизатора: обычный (питоний) и быстрый (на расте). Правда, быстрые написали еще не для всех моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pu6L0lWG-X83",
   "metadata": {
    "id": "Pu6L0lWG-X83",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")      # питоний\n",
    "print(tokenizer)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased\")  # Rust\n",
    "print(tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\") # по умолчанию берет Rust, если тот доступен\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zrPzbBhR-X84",
   "metadata": {
    "id": "zrPzbBhR-X84"
   },
   "outputs": [],
   "source": [
    "# Так мы вызываем токенизатор\n",
    "input_str = \"Hugging Face Transformers is great!\"\n",
    "tokenized_inputs = tokenizer(input_str)\n",
    "\n",
    "\n",
    "print(\"Vanilla Tokenization\")\n",
    "print_encoding(tokenized_inputs)\n",
    "print()\n",
    "\n",
    "# два способа посмотреть (как в pandas):\n",
    "print(tokenized_inputs.input_ids)\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jZHf3Pi_ElGp",
   "metadata": {
    "id": "jZHf3Pi_ElGp"
   },
   "source": [
    "BPE-токенизация в действии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E_8C6L2G-X85",
   "metadata": {
    "id": "E_8C6L2G-X85"
   },
   "outputs": [],
   "source": [
    "cls = [tokenizer.cls_token_id] # в нашем случае 101\n",
    "sep = [tokenizer.sep_token_id] # 102\n",
    "\n",
    "# Токенизация работает в несколько шагов:\n",
    "input_tokens = tokenizer.tokenize(input_str) # собственно токенизировали\n",
    "input_ids = tokenizer.convert_tokens_to_ids(input_tokens) # превратили в айдишники\n",
    "input_ids_special_tokens = cls + input_ids + sep # добавили спецсимволы\n",
    "\n",
    "decoded_str = tokenizer.decode(input_ids_special_tokens) # обратная операция\n",
    "\n",
    "print(\"start:                \", input_str)\n",
    "print(\"tokenize:             \", input_tokens)\n",
    "print(\"convert_tokens_to_ids:\", input_ids)\n",
    "print(\"add special tokens:   \", input_ids_special_tokens)\n",
    "print(\"--------\")\n",
    "print(\"decode:               \", decoded_str)\n",
    "\n",
    "# Внимание: эти шаги не включают создание спецсимволов или аттеншн маски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tdoZ3EEU-X86",
   "metadata": {
    "id": "tdoZ3EEU-X86"
   },
   "outputs": [],
   "source": [
    "# У быстрых токенизаторов свой метод:\n",
    "inputs = tokenizer._tokenizer.encode(input_str)\n",
    "\n",
    "print(input_str)\n",
    "print(\"-\"*5)\n",
    "print(f\"Number of tokens: {len(inputs)}\")\n",
    "print(f\"Ids: {inputs.ids}\")\n",
    "print(f\"Tokens: {inputs.tokens}\")\n",
    "print(f\"Special tokens mask: {inputs.special_tokens_mask}\")\n",
    "print()\n",
    "print(\"char_to_word gives the wordpiece of a character in the input\")\n",
    "char_idx = 8\n",
    "print(f\"For example, the {char_idx + 1}th character of the string is '{input_str[char_idx]}',\"+\\\n",
    "      f\" and it's part of wordpiece {inputs.char_to_token(char_idx)}, '{inputs.tokens[inputs.char_to_token(char_idx)]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vt5WV-6S-X87",
   "metadata": {
    "id": "vt5WV-6S-X87"
   },
   "outputs": [],
   "source": [
    "# Другие фишки:\n",
    "# Токенизатор может возвращать торчевые тензоры (мы это уже видели выше)\n",
    "model_inputs = tokenizer(\"Hugging Face Transformers is great!\", return_tensors=\"pt\")\n",
    "print(\"PyTorch Tensors:\")\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HI3bAzpeoD3_",
   "metadata": {
    "id": "HI3bAzpeoD3_"
   },
   "outputs": [],
   "source": [
    "# Можно передавать список строк и падить их, а заодно обрезать\n",
    "model_inputs = tokenizer([\"Hugging Face Transformers is great!\",\n",
    "                         \"The quick brown fox jumps over the lazy dog.\" +\\\n",
    "                         \"Then the dog got up and ran away because she didn't like foxes.\",\n",
    "                         ],\n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=True,\n",
    "                         truncation=True)\n",
    "print(f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\n",
    "print(\"Padding:\")\n",
    "print_encoding(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "iSZat-nkoD3_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1680524942464,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "iSZat-nkoD3_",
    "outputId": "9410ec69-7d93-4e16-88bc-3937a9ee302a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Decode:\n",
      "['[CLS] Hugging Face Transformers is great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', \"[CLS] The quick brown fox jumps over the lazy dog. Then the dog got up and ran away because she didn't like foxes. [SEP]\"]\n",
      "\n",
      "Batch Decode: (no special characters)\n",
      "['Hugging Face Transformers is great!', \"The quick brown fox jumps over the lazy dog. Then the dog got up and ran away because she didn't like foxes.\"]\n"
     ]
    }
   ],
   "source": [
    "# Можно сразу весь батч отдекодить:\n",
    "print(\"Batch Decode:\")\n",
    "print(tokenizer.batch_decode(model_inputs.input_ids))\n",
    "print()\n",
    "print(\"Batch Decode: (no special characters)\")\n",
    "print(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JmZNLz_noD4A",
   "metadata": {
    "id": "JmZNLz_noD4A"
   },
   "source": [
    "Больше документации тут:\n",
    "[Hugging Face Transformers Docs](https://huggingface.co/docs/transformers/main_classes/tokenizer) и [Hugging Face Tokenizers Library](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html) (про быстрые токенайзеры). Можно даже собственные токенизаторы учить с помощью библиотеки Tokenizers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6juLjnNt-X87",
   "metadata": {
    "id": "6juLjnNt-X87"
   },
   "source": [
    "### 0.2 Модели\n",
    "\n",
    "\n",
    "Инициализация моделей очень похожа на инициализацию токенизаторов. Можно либо использовать специальный класс модели, либо использовать AutoModel, в который впихнуть свою модель. Часто последний метод проще. \n",
    "\n",
    "Большинство предобученных трансформеров имеют похожую архитектуру, плюс в hugginface есть модельки с дополнительными весами (решающий линейный слой сверху), их еще часто называют головами (heads): они заточены решать какие-то конкретные downstream tasks. Hugging Face умеет автоматически выставлять ту архитектуру, которую вам нужно, когда вы определяете класс модели. Например, мы собираемся заняться sentiment analysis, значит, используем `DistilBertForSequenceClassification`. Если бы мы собирались продолжать дообучать берт на задаче masked language modelling, мы бы использовали  `DistilBertForMaskedLM`, а если мы просто хотели заполучить эмбеддинги из берта для какого-то кастомного таска, можно было бы использовать просто `DistilBertModel`.\n",
    "\n",
    "Примерно так это выглядит в схемах: \n",
    "![model_illustration.png](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)\n",
    "\n",
    "\n",
    "Пара примеров задач.\n",
    "```\n",
    "*\n",
    "*ForMaskedLM\n",
    "*ForSequenceClassification\n",
    "*ForTokenClassification\n",
    "*ForQuestionAnswering\n",
    "*ForMultipleChoice\n",
    "...\n",
    "```\n",
    "`*` может быть `AutoModel` или какая-то конкретная модель (например, `DistilBert`)\n",
    "\n",
    "\n",
    "Всего у нас три типа моделей-трансформеров:\n",
    "* Энкодеры (BERT)\n",
    "* Декодеры (GPT)\n",
    "* Энкодер+Декодер (BART или T5)\n",
    "\n",
    "The task-specific classes you have available depend on what type of model you're dealing with.\n",
    "\n",
    "Полный список [тут](https://huggingface.co/docs/transformers/model_doc/auto). Заметьте, что не все модели совместимы со всеми архитектурами, потому что чисто энкодеры решают одни задачи, а чисто декодеры - другие, например. Так, DistilBERT несовместим с архитектурой Seq2Seq, потому что у него нет декодера и masked attention. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RXm1K2sF-X88",
   "metadata": {
    "id": "RXm1K2sF-X88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2) # количество классов в целевой переменной\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1opFV7Vi-X88",
   "metadata": {
    "id": "1opFV7Vi-X88"
   },
   "source": [
    "Предупреждение - абсолютно нормальное, просто говорит нам о том, что веса для sequence classification (верхний навешенный на нашего берта слой) еще не обучены. \n",
    "\n",
    "Передавать инпуты в модель супер просто. Тут есть два варианта, как это сделать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TDZ72k-U-X89",
   "metadata": {
    "id": "TDZ72k-U-X89"
   },
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\") # наш токенизатор возвращает айдишники + attention_mask\n",
    "\n",
    "# Вариант 1 - передаем аргументы явно по ключу\n",
    "model_outputs = model(input_ids=model_inputs.input_ids, attention_mask=model_inputs.attention_mask)\n",
    "\n",
    "# Option 2 - ключи токенизатора совпадают с тем, чего ждет модель, поэтому можем просто распаковать словарь\n",
    "\n",
    "# f({k1: v1, k2: v2}) = f(k1=v1, k2=v2)\n",
    "\n",
    "model_outputs = model(**model_inputs)\n",
    "\n",
    "print(model_inputs)\n",
    "print()\n",
    "print(model_outputs)\n",
    "print()\n",
    "print(f\"Distribution over labels: {torch.softmax(model_outputs.logits, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oRRqhVUloD4C",
   "metadata": {
    "id": "oRRqhVUloD4C"
   },
   "source": [
    "Если вы заметили, в задаче бинарной классификации мы вообще-то могли бы иметь только один аутпут и установить для него порог (типа, выхлоп от 0 до 1 и все, что меньше 0.5 - класс 0). Но так устроены модели huggingface, они по-своему высчитывают loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rvie4gYD-X8-",
   "metadata": {
    "id": "rvie4gYD-X8-"
   },
   "source": [
    "Loss в модель можно передать свой, вычислить его отдельно как `loss_func` и вызвать `loss.backward`. Можно использовать любые оптимизаторы и шедулеры. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Irxo7sDboD4C",
   "metadata": {
    "id": "Irxo7sDboD4C"
   },
   "outputs": [],
   "source": [
    "# Можно так вычислить лосс\n",
    "label = torch.tensor([1])\n",
    "loss = torch.nn.functional.cross_entropy(model_outputs.logits, label)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "# И глянуть параметры\n",
    "list(model.named_parameters())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mpHeG1zDoD4D",
   "metadata": {
    "id": "mpHeG1zDoD4D"
   },
   "source": [
    "Hugging Face модель может лосс непосредственно в аутпуте возвращать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S148gCyG-X8-",
   "metadata": {
    "id": "S148gCyG-X8-"
   },
   "outputs": [],
   "source": [
    "# В таком случае мы должны передать метку класса:\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "model_inputs['labels'] = torch.tensor([1])\n",
    "\n",
    "model_outputs = model(**model_inputs)\n",
    "\n",
    "\n",
    "print(model_outputs)\n",
    "print()\n",
    "print(f\"Model predictions: {labels[model_outputs.logits.argmax()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7Y6E3IxzoD4E",
   "metadata": {
    "id": "7Y6E3IxzoD4E"
   },
   "source": [
    "Можно также доставать из модели скрытые состояния и аттеншны - это полезно для анализа и просто разобрать все на досточки и посмотреть, что там внутри. (Например, [What does BERT look at?](https://arxiv.org/abs/1906.04341))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5WzqhpquoD4E",
   "metadata": {
    "id": "5WzqhpquoD4E"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-cased\", output_attentions=True, output_hidden_states=True) # надо выставить флажки\n",
    "model.eval()\n",
    "\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    model_output = model(**model_inputs)\n",
    "\n",
    "\n",
    "print(\"Hidden state size (per layer):  \", model_output.hidden_states[0].shape)\n",
    "print(\"Attention head size (per layer):\", model_output.attentions[0].shape)     # (layer, batch, query_word_idx, key_word_idxs)\n",
    "                                                                               # y-axis is query, x-axis is key\n",
    "print(model_output)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YQ1Jf-65L39K",
   "metadata": {
    "id": "YQ1Jf-65L39K"
   },
   "source": [
    "Можно их поотрисовывать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SH_MAK-soD4F",
   "metadata": {
    "id": "SH_MAK-soD4F"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(model_inputs.input_ids[0])\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "n_layers = len(model_output.attentions)\n",
    "n_heads = len(model_output.attentions[0][0])\n",
    "fig, axes = plt.subplots(6, 12)\n",
    "fig.set_size_inches(18.5*2, 10.5*2)\n",
    "for layer in range(n_layers):\n",
    "    for i in range(n_heads):\n",
    "        axes[layer, i].imshow(model_output.attentions[layer][0, i])\n",
    "        axes[layer][i].set_xticks(list(range(9)))\n",
    "        axes[layer][i].set_xticklabels(labels=tokens, rotation=\"vertical\")\n",
    "        axes[layer][i].set_yticks(list(range(9)))\n",
    "        axes[layer][i].set_yticklabels(labels=tokens)\n",
    "\n",
    "        if layer == 5:\n",
    "            axes[layer, i].set(xlabel=f\"head={i}\")\n",
    "        if i == 0:\n",
    "            axes[layer, i].set(ylabel=f\"layer={layer}\")\n",
    "            \n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uumcErs2-X80",
   "metadata": {
    "id": "uumcErs2-X80"
   },
   "source": [
    "## Часть 1: Finetuning\n",
    "\n",
    "Скорее всего, нам захочется модель подучить. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WDdGp4Ua-X81",
   "metadata": {
    "id": "WDdGp4Ua-X81"
   },
   "source": [
    "### 2.1 Загрузка датасета\n",
    "\n",
    "В дополнение к моделям, [huggingface](https://huggingface.co/datasets) имеет и датасеты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OTsW-Wwi-X81",
   "metadata": {
    "id": "OTsW-Wwi-X81"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "\n",
    "# Возьмем только первые 50 токенов для скорости\n",
    "def truncate(example):\n",
    "    return {\n",
    "        'text': \" \".join(example['text'].split()[:50]),\n",
    "        'label': example['label']\n",
    "    }\n",
    "\n",
    "# Возьмем 128 рандомных примеров для трейна и 32 для валидейшна\n",
    "small_imdb_dataset = DatasetDict(\n",
    "    train=imdb_dataset['train'].shuffle(seed=1111).select(range(128)).map(truncate),\n",
    "    val=imdb_dataset['train'].shuffle(seed=1111).select(range(128, 160)).map(truncate),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vCaX-gNo0OEV",
   "metadata": {
    "id": "vCaX-gNo0OEV"
   },
   "outputs": [],
   "source": [
    "small_imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bBS4c44A-X82",
   "metadata": {
    "id": "bBS4c44A-X82"
   },
   "outputs": [],
   "source": [
    "small_imdb_dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bjqop3N-X8_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "0e9631de31564fa1ad7a53a872fd5976",
      "1d3f8564bd0a4d8494ac7fbaacedf256",
      "a3946fcac4964807a9beba16b867dfba",
      "40e0562dba404108a49faead9d9140e4",
      "1f8fd070a5b8496ea302e236db619f6b",
      "5b855eaa15354c578ca4c8ddfba5b124",
      "d3af7d2ec7ab48bf893b4ed390de1011",
      "bbb57ec768ec436584be3613fe07c81d",
      "af0352113d8f48d1a852bbefac8fb267",
      "398634f73dae4288b0fff1de90055f53",
      "aa3b32b4047f4274ad80d063fd4e7e3f",
      "45c20042466441859b0ae8fb5aa58442",
      "5d18299109ae4aa0b24df429492cb02b",
      "6bf62d5b109248a9bd9635891f4bd6b2",
      "98a4ff1411724a86a1ccc0e64b3106e2",
      "ebf8887eaec343f499a5c4d9c947ee3f",
      "3dfd673f8a4b46fbabd64a37fad17792",
      "3dd6efac199940379a8c17a6ee42bf57",
      "61d89e5e80774af3b150b1a796f099fe",
      "64773289e5574b279b6742515c436357",
      "4f98e3f2b0ad43f892f2d47010001bc1",
      "ef9d93859c7e4cdabd9a3c6c75034051"
     ]
    },
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1680526880880,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "3bjqop3N-X8_",
    "outputId": "f24fb69a-c288-4f8c-8942-efae752007d4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9631de31564fa1ad7a53a872fd5976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c20042466441859b0ae8fb5aa58442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Подготовим датасет - это токенизирует его и делит по батчам на 16 сэмплов\n",
    "small_tokenized_dataset = small_imdb_dataset.map(\n",
    "    lambda example: tokenizer(example['text'], padding=True, truncation=True),\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "small_tokenized_dataset = small_tokenized_dataset.remove_columns([\"text\"])\n",
    "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "small_tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450eUYlf-X8_",
   "metadata": {
    "id": "450eUYlf-X8_"
   },
   "outputs": [],
   "source": [
    "small_tokenized_dataset['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "Q3e7_htt-X8_",
   "metadata": {
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1680526970475,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "Q3e7_htt-X8_"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=16)\n",
    "eval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qRm6Tw_z-X8-",
   "metadata": {
    "id": "qRm6Tw_z-X8-"
   },
   "source": [
    "### 2.2 Обучение\n",
    "\n",
    "Чтобы учить свои модельки, вы можете просто использовать стандартный трейнлуп ванильного торча. Модели Hugging Face - это тоже класс `torch.nn.Module`, так что бэкпроп работает ровно так же, и можно использовать те же оптимизаторы, что и обычно. Hugging Face также включает собственные оптимизаторы и шедулеры, которые использовались при обучении трансформеров, так что их тоже можно юзать. \n",
    "\n",
    "Для оптимизации будем использовать AdamW Optimizer - это Адам с плюшкой в виде weight decay. Также подключим линейный шедулер, который будет уменьшать lr понемножку после каждого шага трейнлупа. \n",
    "\n",
    "Есть и другие оптимизаторы и шедулеры, эти - дефолтные. Можно посмотреть на эти: [Hugging Face offers](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#schedules), или стандартные тут: [Pytorch](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) (например, [ReduceLROnPlateau](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html), который начинает снижать lr только тогда, когда val loss перестает падать), или даже собственные написать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "xoSLwc2eZaa_",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1680527230812,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "xoSLwc2eZaa_"
   },
   "outputs": [],
   "source": [
    "!mkdir checkpoints # создадим папку, куда торч будет сбрасывать чекпойнты - а то вдруг у вас отрубят свет....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A7CvjTIv-X9A",
   "metadata": {
    "id": "A7CvjTIv-X9A"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = 3 * len(train_dataloader)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    model.train()\n",
    "    for batch_i, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # batch = ([text1, text2], [0, 1])\n",
    "\n",
    "        output = model(**batch) # та самая распаковка инпутов и аттеншна\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output.loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    for batch_i, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = model(**batch)\n",
    "        loss += output.loss\n",
    "    \n",
    "    avg_val_loss = loss / len(eval_dataloader)\n",
    "    print(f\"Validation loss: {avg_val_loss}\")\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(\"Saving checkpoint!\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "            },\n",
    "            f\"checkpoints/epoch_{epoch}.pt\"\n",
    "        )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YrfE9c04-X9A",
   "metadata": {
    "id": "YrfE9c04-X9A"
   },
   "source": [
    "Еще у Hugging Face есть свой собственный класс для обучения, похожий на штуки, как в Lightning (если вы его еще не смотрели, посмотрите). Можно не писать свои трейнлупы за пупы!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RULukU5eoD4M",
   "metadata": {
    "id": "RULukU5eoD4M"
   },
   "source": [
    "`TrainingArguments` определяет различные параметры обучения, типа как часто оценивать и бэкапить модельки, куда их бэкапить и т.п. Настраивать можно много всего, можно посмотреть [тут](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments). Некоторые подконтрольные вещи включают:\n",
    "* learning rate, weight decay, gradient clipping, \n",
    "* checkpointing, logging, and evaluation frequency\n",
    "* where you log to (default is tensorboard, but if you use WandB or MLFlow they have integrations)\n",
    "\n",
    "Класс `Trainer` собственно обучает модель. Можно в него передать `TrainingArguments`, модель, датасеты, токенизатор, оптимайзер и даже чекпойнты модели, если у вас все-таки отрубили свет... Функция `compute_metrics` вызывается в конце evaluation/validation, чтобы посчитать метрики. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FEmERCu_-X9B",
   "metadata": {
    "id": "FEmERCu_-X9B",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_hf_trainer\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\", # валидация в конце каждой эпохи\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=224\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # calculates the accuracy\n",
    "    return {\"accuracy\": np.mean(predictions == labels)}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=small_tokenized_dataset['train'],\n",
    "    eval_dataset=small_tokenized_dataset['val'], # для финальной оценки меняем на test\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MllSTgehoD4O",
   "metadata": {
    "id": "MllSTgehoD4O"
   },
   "source": [
    "Некоторые практические советы для файнтьюнинга:\n",
    "\n",
    "**Хорошие дефолтные параметры.** Гиперпараметры зависят от задачи и датасета. Хорошо бы погридсерчить на лучшие, но есть практикой установленные хорошие стартовые значения. \n",
    "* Эпохи: {2, 3, 4} (чем больше данных, тем меньше надо эпох)\n",
    "* Размер батча (чем больше, тем быстрее и стабильнее)\n",
    "* Оптимизатор: AdamW\n",
    "* AdamW learning rate: {2e-5, 5e-5}\n",
    "* Learning rate scheduler: linear warm up for first {0, 100, 500} steps of training\n",
    "* weight_decay (l2 regularization): {0, 0.01, 0.1}\n",
    "\n",
    "Нужно следить за validation loss, чтобы понять, какие параметры хорошие. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tHI3KuNZ-X8w",
   "metadata": {
    "id": "tHI3KuNZ-X8w"
   },
   "source": [
    "## Приложение: Пайплайны\n",
    "\n",
    "Для некоторых стандартных NLP задач, типа sentiment classification или QA, есть уже полностью готовые модели, которые доступны в интерфейсе [Pipeline](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline) Hugging Face. \n",
    "\n",
    "Они, может быть, менее интересны, но знать про них стоит. \n",
    "\n",
    "Вот примерчик:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "gOj5ODS0-X8x",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11934,
     "status": "ok",
     "timestamp": 1680528011009,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "gOj5ODS0-X8x"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D5wZuMG2-X8y",
   "metadata": {
    "id": "D5wZuMG2-X8y"
   },
   "source": [
    "Пайплайн просто гоняется по строке инпута"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GrygLkiQ-X8y",
   "metadata": {
    "id": "GrygLkiQ-X8y"
   },
   "outputs": [],
   "source": [
    "sentiment_analysis(\"Hugging Face Transformers is really cool!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2E8qKH-X8z",
   "metadata": {
    "id": "0e2E8qKH-X8z"
   },
   "source": [
    "Или списке строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EpBvCpVM-X8z",
   "metadata": {
    "id": "EpBvCpVM-X8z"
   },
   "outputs": [],
   "source": [
    "sentiment_analysis([\"I didn't know if I would like Hákarl, but it turned out pretty good.\",\n",
    "                    \"I didn't know if I would like Hákarl, and it was just as bad as I'd heard.\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e9631de31564fa1ad7a53a872fd5976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d3f8564bd0a4d8494ac7fbaacedf256",
       "IPY_MODEL_a3946fcac4964807a9beba16b867dfba",
       "IPY_MODEL_40e0562dba404108a49faead9d9140e4"
      ],
      "layout": "IPY_MODEL_1f8fd070a5b8496ea302e236db619f6b"
     }
    },
    "1d3f8564bd0a4d8494ac7fbaacedf256": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b855eaa15354c578ca4c8ddfba5b124",
      "placeholder": "​",
      "style": "IPY_MODEL_d3af7d2ec7ab48bf893b4ed390de1011",
      "value": "Map:  12%"
     }
    },
    "1f8fd070a5b8496ea302e236db619f6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "398634f73dae4288b0fff1de90055f53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3dd6efac199940379a8c17a6ee42bf57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3dfd673f8a4b46fbabd64a37fad17792": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40e0562dba404108a49faead9d9140e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_398634f73dae4288b0fff1de90055f53",
      "placeholder": "​",
      "style": "IPY_MODEL_aa3b32b4047f4274ad80d063fd4e7e3f",
      "value": " 16/128 [00:00&lt;00:00, 143.81 examples/s]"
     }
    },
    "45c20042466441859b0ae8fb5aa58442": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d18299109ae4aa0b24df429492cb02b",
       "IPY_MODEL_6bf62d5b109248a9bd9635891f4bd6b2",
       "IPY_MODEL_98a4ff1411724a86a1ccc0e64b3106e2"
      ],
      "layout": "IPY_MODEL_ebf8887eaec343f499a5c4d9c947ee3f"
     }
    },
    "4f98e3f2b0ad43f892f2d47010001bc1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b855eaa15354c578ca4c8ddfba5b124": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d18299109ae4aa0b24df429492cb02b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3dfd673f8a4b46fbabd64a37fad17792",
      "placeholder": "​",
      "style": "IPY_MODEL_3dd6efac199940379a8c17a6ee42bf57",
      "value": "Map:   0%"
     }
    },
    "61d89e5e80774af3b150b1a796f099fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64773289e5574b279b6742515c436357": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6bf62d5b109248a9bd9635891f4bd6b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61d89e5e80774af3b150b1a796f099fe",
      "max": 32,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_64773289e5574b279b6742515c436357",
      "value": 32
     }
    },
    "98a4ff1411724a86a1ccc0e64b3106e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f98e3f2b0ad43f892f2d47010001bc1",
      "placeholder": "​",
      "style": "IPY_MODEL_ef9d93859c7e4cdabd9a3c6c75034051",
      "value": " 0/32 [00:00&lt;?, ? examples/s]"
     }
    },
    "a3946fcac4964807a9beba16b867dfba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbb57ec768ec436584be3613fe07c81d",
      "max": 128,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af0352113d8f48d1a852bbefac8fb267",
      "value": 128
     }
    },
    "aa3b32b4047f4274ad80d063fd4e7e3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af0352113d8f48d1a852bbefac8fb267": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bbb57ec768ec436584be3613fe07c81d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3af7d2ec7ab48bf893b4ed390de1011": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebf8887eaec343f499a5c4d9c947ee3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "ef9d93859c7e4cdabd9a3c6c75034051": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
