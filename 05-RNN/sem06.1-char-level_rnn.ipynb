{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"1W8R8WgZceEk"},"source":["# Character-Level LSTM\n","На этом семинаре поговорим про рекуррентные нейронные сети (Recurrent Neural Networ, RNN). Мы обучим модель на тексте книги \"Анна Каренина\", после чего попробуем генерировать новый текст.\n","\n","**Модель сможет генерировать новый текст на основе текста \"Анны Карениной\"!**\n","\n","Можно посмотреть полезную [статью про RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) и [реализацию в Torch](https://github.com/karpathy/char-rnn). \n","\n","Ообщая архитектура RNN:\n","\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"500\">"]},{"cell_type":"code","metadata":{"id":"sqUOE2flceEl"},"source":["import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_wHfCDyzceEl"},"source":["## Загрузим данные\n","\n","Загрузим текстовый файл \"Анны Карениной\"."]},{"cell_type":"code","metadata":{"id":"b34kfqIOceEl"},"source":["# open text file and read in data as `text`\n","with open('anna.txt', 'r') as f:\n","    text = f.read()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jp1Ljc4mceEl"},"source":["Посмотрим первые 100 символов:"]},{"cell_type":"code","metadata":{"id":"7VctmLQfceEl"},"source":["text[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4iC21bopceEl"},"source":["### Токенизация\n","\n","В ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно. Кодирование символов как целых чисел упрощает их использование в качестве входных данных в сети."]},{"cell_type":"code","metadata":{"id":"tYVlmnxLceEl"},"source":["chars = tuple(set(text))\n","int2char = dict(enumerate(chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}\n","\n","encoded = np.array([char2int[ch] for ch in text])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJIzwzSwceEl"},"source":["Посмотрим как символы закодировались целыми числами"]},{"cell_type":"code","metadata":{"id":"WK1MYr_9ceEl"},"source":["encoded[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"azltQy-gceEl"},"source":["## Предобработка данных\n","\n","Как можно видеть на изображении char-RNN выше, сеть ожидает **one-hot encoded** входа, что означает, что каждый символ преобразуется в целое число (через созданный словарь), а затем преобразуется в вектор-столбец, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте создадим для этого функцию."]},{"cell_type":"code","metadata":{"id":"OnahALhiceEl"},"source":["def one_hot_encode(arr, n_labels):\n","    \n","    # Initialize the the encoded array\n","    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n","    \n","    # Fill the appropriate elements with ones\n","    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n","    \n","    # Finally reshape it to get back to the original array\n","    one_hot = one_hot.reshape((*arr.shape, n_labels))\n","    \n","    return one_hot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3lTdLKfceEl"},"source":["# check that the function works as expected\n","test_seq = np.array([[3, 5, 1]])\n","one_hot = one_hot_encode(test_seq, 8)\n","\n","print(one_hot)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9YyL91CuceEl"},"source":["## Создаем mini-batch'и\n","\n","\n","Создатдим мини-батчи для обучения. На простом примере они будут выглядеть так:\n","\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n","<br>\n","\n","Возьмем закодированные символы (переданные как параметр `arr`) и разделим их на несколько последовательностей, заданных параметром `batch_size`. Каждая из наших последовательностей будет иметь длину `seq_length`.\n","\n","### Создание батчей\n","\n","**1. Первое, что нам нужно сделать, это отбросить часть текста, чтобы у нас были только полные мини-батчи**\n","\n","Каждый батч содержит $ N\\times M $ символов, где $ N $ - это размер батча (количество последовательностей в батче), а $ M $ - длина `seq_length` или количество шагов в последовательности. Затем, чтобы получить общее количество батчей $ K $, которое мы можем сделать из массива `arr`, нужно разделить длину `arr` на количество символов в батче. Когда мы узнаем количество батчей, можно получить общее количество символов, которые нужно сохранить, из `arr`: $ N * M * K $.\n","\n","**2. После этого нам нужно разделить `arr` на $N$ батчей** \n","\n","Это можно сделать с помощью `arr.reshape(size)`, где `size` - это кортеж, содержащий размеры измененного массива. Мы знаем, что нам нужно $ N $ последовательностей в батче, поэтому сделаем его размером первого измерения. Для второго измерения можем использовать «-1» в качестве заполнителя, он заполнит массив соответствующими данными. После этого должен остаться массив $N\\times(M * K)$.\n","\n","**3. Теперь, когда у нас есть этот массив, мы можем перебирать его, чтобы получить наши мини-батчи**\n","\n","Идея состоит в том, что каждая партия представляет собой окно $ N\\times M $ в массиве $ N\\times (M * K) $. Для каждого последующего батча окно перемещается на `seq_length`. Мы также хотим создать как входной, так и выходной массивы. Это окно можно сделать с помощью `range`, чтобы делать шаги размером `n_steps` от $ 0 $ до `arr.shape [1]`, общее количество токенов в каждой последовательности. Таким образом, целые числа, которые получены из диапазона, всегда указывают на начало батча, и каждое окно имеет ширину `seq_length`."]},{"cell_type":"code","metadata":{"id":"2ECftYejnvpx"},"source":["def get_batches(arr, batch_size, seq_length):\n","    '''Create a generator that returns batches of size\n","       batch_size x seq_length from arr.\n","       \n","       Arguments\n","       ---------\n","       arr: Array you want to make batches from\n","       batch_size: Batch size, the number of sequences per batch\n","       seq_length: Number of encoded chars in a sequence\n","    '''\n","    \n","    batch_size_total = batch_size * seq_length\n","    # total number of batches we can make\n","    n_batches = len(arr)//batch_size_total\n","    \n","    # Keep only enough characters to make full batches\n","    arr = arr[:n_batches * batch_size_total]\n","    # Reshape into batch_size rows\n","    arr = arr.reshape((batch_size, -1))\n","    \n","    # iterate through the array, one sequence at a time\n","    for n in range(0, arr.shape[1], seq_length):\n","        # The features\n","        x = arr[:, n:n+seq_length]\n","        # The targets, shifted by one\n","        y = np.zeros_like(x)\n","        try:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n","        except IndexError:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","        yield x, y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s9uKOvbqceEl"},"source":["### Протестируем\n","\n","Теперь создадим несколько наборов данных, и проверим, что происходит, когда мы создаем батчи."]},{"cell_type":"code","metadata":{"id":"qtKlLXi1ceEl"},"source":["batches = get_batches(encoded, 8, 50)\n","x, y = next(batches)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rg5MUTqqceEl"},"source":["# printing out the first 10 items in a sequence\n","print('x\\n', x[:10, :10])\n","print('\\ny\\n', y[:10, :10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jouxv0L2ceEl"},"source":["---\n","## Зададим архитектуру\n","\n","\n","<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=500px>"]},{"cell_type":"markdown","metadata":{"id":"E7s5eRaoceEl"},"source":["### Структура модели\n","\n","В `__init__` предлагаемая структура выглядит следующим образом:\n","* Создаём и храним необходимые словари (уже реализовано)\n","* Определяем слой LSTM, который принимает в качестве параметров: размер ввода (количество символов), размер скрытого слоя `n_hidden`, количество слоев` n_layers`, вероятность drop-out'а `drop_prob` и логическое значение batch_first (True)\n","* Определяем слой drop-out с помощью drop_prob\n","* Определяем полносвязанный слой с параметрами: размер ввода `n_hidden` и размер выхода - количество символов\n","* Наконец, инициализируем веса\n","\n","Обратите внимание, что некоторые параметры были названы и указаны в функции `__init__`, их нужно сохранить и использовать, выполняя что-то вроде` self.drop_prob = drop_prob`."]},{"cell_type":"markdown","metadata":{"id":"Plm1atCuceEl"},"source":["---\n","### Входы-выходы LSTM\n","\n","Вы можете создать [LSTM layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) следующим образом\n","\n","```python\n","self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n","                            dropout=drop_prob, batch_first=True)\n","```\n","\n","где `input_siz`e - это количество символов, которые эта ячейка ожидает видеть в качестве последовательного ввода, а `n_hidden` - это количество элементов в скрытых слоях ячейки. Можно добавить drop-out, добавив параметр `dropout` с заданной вероятностью. Наконец, в функции `forward` мы можем складывать ячейки LSTM в слои, используя `.view`.\n","\n","Также требуется создать начальное скрытое состояние всех нулей:\n","\n","```python\n","self.init_hidden()\n","```"]},{"cell_type":"code","metadata":{"id":"HlTnDntHceEl"},"source":["# check if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","if(train_on_gpu):\n","    print('Training on GPU!')\n","else: \n","    print('No GPU available, training on CPU; consider making n_epochs very small.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VPq1EA38rBqn"},"source":["class CharRNN(nn.Module):\n","    \n","    def __init__(self, tokens, n_hidden=256, n_layers=2,\n","                               drop_prob=0.5, lr=0.001):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.n_layers = n_layers\n","        self.n_hidden = n_hidden\n","        self.lr = lr\n","        \n","        # creating character dictionaries\n","        self.chars = tokens\n","        self.int2char = dict(enumerate(self.chars))\n","        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","        \n","        ## define the LSTM\n","        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n","                            dropout=drop_prob, batch_first=True)\n","        \n","        ## define a dropout layer\n","        self.dropout = nn.Dropout(drop_prob)\n","        \n","        ## define the final, fully-connected output layer\n","        self.fc = nn.Linear(n_hidden, len(self.chars))\n","      \n","    \n","    def forward(self, x, hidden):\n","        ''' Forward pass through the network. \n","            These inputs are x, and the hidden/cell state `hidden`. '''\n","                \n","        ## Get the outputs and the new hidden state from the lstm\n","        r_output, hidden = self.lstm(x, hidden)\n","        \n","        ## pass through a dropout layer\n","        out = self.dropout(r_output)\n","        \n","        # Stack up LSTM outputs using view\n","        # you may need to use contiguous to reshape the output\n","        out = out.contiguous().view(-1, self.n_hidden)\n","        \n","        ## put x through the fully-connected layer\n","        out = self.fc(out)\n","        \n","        # return the final output and the hidden state\n","        return out, hidden\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","        \n","        return hidden"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5IrBRlEPceEl"},"source":["## Обучим модель\n","\n","Во время обучения нужно установить количество эпох, скорость обучения и другие параметры.\n","\n","Используем оптимизатор Adam и кросс-энтропию, считаем loss и, как обычно, выполняем back propagation.\n","\n","Пара подробностей об обучении:\n","> * Во время цикла мы отделяем скрытое состояние от его истории; на этот раз установив его равным новой переменной * tuple *, потому что скрытое состояние LSTM, является кортежем скрытых состояний.\n","* Мы используем [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) чтобы избавиться от взрывающихся градиентов."]},{"cell_type":"code","metadata":{"id":"lv8VkRI0ceEl"},"source":["def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n","    ''' Training a network \n","    \n","        Arguments\n","        ---------\n","        \n","        net: CharRNN network\n","        data: text data to train the network\n","        epochs: Number of epochs to train\n","        batch_size: Number of mini-sequences per mini-batch, aka batch size\n","        seq_length: Number of character steps per mini-batch\n","        lr: learning rate\n","        clip: gradient clipping\n","        val_frac: Fraction of data to hold out for validation\n","        print_every: Number of steps for printing training and validation loss\n","    \n","    '''\n","    net.train()\n","    \n","    opt = torch.optim.Adam(net.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    # create training and validation data\n","    val_idx = int(len(data)*(1-val_frac))\n","    data, val_data = data[:val_idx], data[val_idx:]\n","    \n","    if(train_on_gpu):\n","        net.cuda()\n","    \n","    counter = 0\n","    n_chars = len(net.chars)\n","    for e in range(epochs):\n","        # initialize hidden state\n","        h = net.init_hidden(batch_size)\n","        \n","        for x, y in get_batches(data, batch_size, seq_length):\n","            counter += 1\n","            \n","            # One-hot encode our data and make them Torch tensors\n","            x = one_hot_encode(x, n_chars)\n","            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n","            \n","            if(train_on_gpu):\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","\n","            # zero accumulated gradients\n","            net.zero_grad()\n","            \n","            # get the output from the model\n","            output, h = net(inputs, h)\n","            \n","            # calculate the loss and perform backprop\n","            loss = criterion(output, targets.view(batch_size*seq_length).long())\n","            loss.backward()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(net.parameters(), clip)\n","            opt.step()\n","            \n","            # loss stats\n","            if counter % print_every == 0:\n","                # Get validation loss\n","                val_h = net.init_hidden(batch_size)\n","                val_losses = []\n","                net.eval()\n","                for x, y in get_batches(val_data, batch_size, seq_length):\n","                    # One-hot encode our data and make them Torch tensors\n","                    x = one_hot_encode(x, n_chars)\n","                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n","                    \n","                    # Creating new variables for the hidden state, otherwise\n","                    # we'd backprop through the entire training history\n","                    val_h = tuple([each.data for each in val_h])\n","                    \n","                    inputs, targets = x, y\n","                    if(train_on_gpu):\n","                        inputs, targets = inputs.cuda(), targets.cuda()\n","\n","                    output, val_h = net(inputs, val_h)\n","                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n","                \n","                    val_losses.append(val_loss.item())\n","                \n","                net.train() # reset to train mode after iterationg through validation data\n","                \n","                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                      \"Step: {}...\".format(counter),\n","                      \"Loss: {:.4f}...\".format(loss.item()),\n","                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gt0q4KGEceEm"},"source":["## Определим модель\n","\n","Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей."]},{"cell_type":"code","metadata":{"id":"ykMcIloEr3G7"},"source":["# define and print the net\n","n_hidden=512\n","n_layers=2\n","\n","net = CharRNN(chars, n_hidden, n_layers)\n","print(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHy6mECuceEm"},"source":["### Установим гиперпараметры"]},{"cell_type":"code","metadata":{"id":"8hTkNrWEsjgI"},"source":["batch_size = 128\n","seq_length = 100\n","n_epochs = 20 # start smaller if you are just testing initial behavior\n","\n","# train the model\n","train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZfZxvNoDceEm"},"source":["## Checkpoint\n","\n","После обучения сохраним модель, чтобы можно было загрузить ее позже. Здесь сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."]},{"cell_type":"code","metadata":{"id":"q6RXl5VAceEm"},"source":["# change the name, for saving multiple files\n","model_name = 'rnn_x_epoch.net'\n","\n","checkpoint = {'n_hidden': net.n_hidden,\n","              'n_layers': net.n_layers,\n","              'state_dict': net.state_dict(),\n","              'tokens': net.chars}\n","\n","with open(model_name, 'wb') as f:\n","    torch.save(checkpoint, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2sJhx5iceEm"},"source":["---\n","## Делаем предсказания\n","\n","Теперь, когда мы обучили модель, сделаем предсказание следующих символов! Для предсказания мы передаем последний символ, и сеть предсказывает следующий символ, который мы потом передаем снова на вхол и получаем еще один предсказанный символ и так далее...\n","\n","Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые наиболее вероятные символы $K$. Это не позволит сети выдавать нам совершенно абсурдные прогнозы, а также позволит внести некоторый шум и случайность в выбранный текст. Узнать больше [можно здесь](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."]},{"cell_type":"code","metadata":{"id":"QEIRW_B2ceEm"},"source":["def predict(net, char, h=None, top_k=None):\n","        ''' Given a character, predict the next character.\n","            Returns the predicted character and the hidden state.\n","        '''\n","        \n","        # tensor inputs\n","        x = np.array([[net.char2int[char]]])\n","        x = one_hot_encode(x, len(net.chars))\n","        inputs = torch.from_numpy(x)\n","        \n","        if(train_on_gpu):\n","            inputs = inputs.cuda()\n","        \n","        # detach hidden state from history\n","        h = tuple([each.data for each in h])\n","        # get the output of the model\n","        out, h = net(inputs, h)\n","\n","        # get the character probabilities\n","        p = F.softmax(out, dim=1).data\n","        if(train_on_gpu):\n","            p = p.cpu() # move to cpu\n","        \n","        # get top characters\n","        if top_k is None:\n","            top_ch = np.arange(len(net.chars))\n","        else:\n","            p, top_ch = p.topk(top_k)\n","            top_ch = top_ch.numpy().squeeze()\n","        \n","        # select the likely next character with some element of randomness\n","        p = p.numpy().squeeze()\n","        char = np.random.choice(top_ch, p=p/p.sum())\n","        \n","        # return the encoded value of the predicted char and the hidden state\n","        return net.int2char[char], h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OG38j3gQceEm"},"source":["### Priming и генерирование текста\n","\n","Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы. "]},{"cell_type":"code","metadata":{"id":"P9vpB5gRceEm"},"source":["def sample(net, size, prime='Анна', top_k=None):\n","        \n","    if(train_on_gpu):\n","        net.cuda()\n","    else:\n","        net.cpu()\n","    \n","    net.eval() # eval mode\n","    \n","    # First off, run through the prime characters\n","    chars = [ch for ch in prime]\n","    h = net.init_hidden(1)\n","    for ch in prime:\n","        char, h = predict(net, ch, h, top_k=top_k)\n","\n","    chars.append(char)\n","    \n","    # Now pass in the previous character and get a new one\n","    for ii in range(size):\n","        char, h = predict(net, chars[-1], h, top_k=top_k)\n","        chars.append(char)\n","\n","    return ''.join(chars)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BqmFA9eEceEm"},"source":["print(sample(net, 1000, prime='Анна', top_k=5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"942mjdQHceEm"},"source":["## Loading a checkpoint"]},{"cell_type":"code","metadata":{"id":"Xt9ldUuSceEm"},"source":["# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n","with open('rnn_x_epoch.net', 'rb') as f:\n","    checkpoint = torch.load(f)\n","    \n","loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n","loaded.load_state_dict(checkpoint['state_dict'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ut6R3zDcceEm"},"source":["# Sample using a loaded model\n","print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rhp9qC8neCk"},"source":["### Полезные ссылки:\n","\n","\n","*   [Блог-пост Christopher'а Olah'а по LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"]},{"cell_type":"code","metadata":{"id":"A5Twt8oBnell"},"source":[],"execution_count":null,"outputs":[]}]}