{"cells":[{"cell_type":"markdown","source":["Установим torchmetrics, чтобы руками не считать точность, и склонируем с гитхаба датасет."],"metadata":{"id":"jA4Uhu1WkPqR"}},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"id":"_9eh4gJ-jVtk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/fortvivlan/catset.git\n","%cd /content/catset"],"metadata":{"id":"GgTq5Z78jHpg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQqRUpjdi6P1"},"source":["Imports"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"2kXlTQG2i6P1","executionInfo":{"status":"ok","timestamp":1697964223757,"user_tz":-180,"elapsed":6062,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as opt\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torchvision import datasets\n","from torchvision.transforms import Compose, Normalize, ToTensor\n","from torch.utils.data import Subset\n","from torchmetrics import Accuracy as VAccuracy\n","from sklearn.model_selection import train_test_split\n","from tqdm.auto import tqdm\n","import warnings\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"ZIgJ3iaxi6P2"},"source":["Теперь нам нужно подготовить датасет к работе. Картинки неплохо бы нормализовать: помним, что любая картинка эссеншали только матрица с чиселками. Сделаем наши картинки не слишком яркими, чтобы все они были примерно одинаковыми по интенсивности цвета и подобное. Для этого нужно предпосчитать среднее и стандартное отклонение по датасету:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"wIC8EPj_i6P2","executionInfo":{"status":"ok","timestamp":1697964226356,"user_tz":-180,"elapsed":222,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["def get_mean_and_std(dataloader):\n","    '''Считаем среднее и стандартное отклонение'''\n","    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n","    for data, _ in dataloader: # data - батч с картинками, _ - y_true, но они нам щас не нужны\n","        channels_sum += torch.mean(data, dim=[0, 2, 3]) # [batch_size x 3 x image_size x image_size]\n","        channels_squared_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n","        num_batches += 1\n","\n","    mean = channels_sum / num_batches\n","\n","    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n","\n","    return mean, std"]},{"cell_type":"markdown","source":["Заготовим трансформацию картинки: мы должны будем все картинки в батче превратить в тензоры (чтобы потом по ним рассчитать их среднее и ст. отклонение.\n","\n","Торч умеет работать с датасетами прямо из папки: для картинок это очень удобно, потому что иначе вам пришлось бы загружать все свои тяжелые картинки в оперативную память, было бы медленно и тупо."],"metadata":{"id":"n_RWdr2VmLyo"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"3n-iiipWi6P2","executionInfo":{"status":"ok","timestamp":1697964227934,"user_tz":-180,"elapsed":232,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["transform = Compose(\n","    [ToTensor()]\n",")\n","\n","# загрузим просто все картинки махом (хотя вообще-то нормализовать надо только по трейну, но ладно, мы учимся)\n","dataset = torch.utils.data.DataLoader(datasets.ImageFolder('Cats', transform=transform), batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"1u2xAd7Hi6P2","outputId":"9a0d2838-cebf-4f72-f0ce-c30826fb449a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697964230211,"user_tz":-180,"elapsed":630,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.3851, 0.3576, 0.3296]) tensor([0.2769, 0.2711, 0.2644])\n"]}],"source":["mean, std = get_mean_and_std(dataset)\n","print(mean, std)"]},{"cell_type":"markdown","metadata":{"id":"cwT6yH4Hi6P2"},"source":["Получили нужные чиселки: теперь нужно добавить их в трансформацию, которую будем использовать уже в реальной подготовке датасета."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"z1njG_Mji6P2","executionInfo":{"status":"ok","timestamp":1697964232521,"user_tz":-180,"elapsed":1,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["transform = Compose(\n","    [\n","        ToTensor(),\n","        Normalize((0.3851, 0.3576, 0.3296), (0.2769, 0.2711, 0.2644))\n","    ]\n",")\n","dataset = datasets.ImageFolder('Cats', transform=transform)"]},{"cell_type":"markdown","source":["Напишем функцию, которая будет сплитить наш датасет: сперва отшаффлим индексы и разделим их стандартной ск-лерновской тулзой, а потом воспользуемся утилитой торча Subset."],"metadata":{"id":"At-hqwdfnF0p"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"jpferLrIi6P3","executionInfo":{"status":"ok","timestamp":1697964234755,"user_tz":-180,"elapsed":226,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["def train_val_dataset(dataset, val_split=0.25):\n","    '''Create train and validation datasets'''\n","    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n","    datasets = {}\n","    datasets['train'] = Subset(dataset, train_idx)\n","    datasets['val'] = Subset(dataset, val_idx)\n","    return datasets"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"4L1SYvDGi6P3","executionInfo":{"status":"ok","timestamp":1697964236815,"user_tz":-180,"elapsed":247,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["traintest = train_val_dataset(dataset)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Lmi9Hq03i6P3","executionInfo":{"status":"ok","timestamp":1697964237927,"user_tz":-180,"elapsed":1,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["train_dataloader = torch.utils.data.DataLoader(traintest['train'], batch_size=16, shuffle=True)\n","val_dataloader = torch.utils.data.DataLoader(traintest['val'], batch_size=16, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"rXDdtkeoi6P3"},"source":["Ну вот, теперь наш датасет готов, пора писать трейнлуп и архитектурку."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"qVIFviE1i6P3","executionInfo":{"status":"ok","timestamp":1697964240243,"user_tz":-180,"elapsed":229,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["def train(model, optimizer, n_epochs=5):\n","    for epoch in range(1, n_epochs + 1):\n","        # train\n","        for x_train, y_train in tqdm(train_dataloader):\n","            y_pred = model(x_train)\n","            loss = F.cross_entropy(y_pred, y_train) # используем кросс-энтропию\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        # validation\n","        if not epoch % 2:\n","            val_loss = []\n","            val_accuracy = []\n","            with torch.no_grad():\n","                for x_val, y_val in tqdm(val_dataloader):\n","                    y_pred = model(x_val)\n","                    loss = F.cross_entropy(y_pred, y_val)\n","                    val_loss.append(loss.numpy())\n","                    val_accuracy.extend((torch.argmax(y_pred, dim=-1) == y_val).numpy().tolist())\n","\n","            print(f\"Epoch: {epoch}\\tloss: {np.mean(val_loss)}\\taccuracy: {np.mean(val_accuracy)}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"skVIbrFFi6P3","executionInfo":{"status":"ok","timestamp":1697964242502,"user_tz":-180,"elapsed":222,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["class Torchic(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # VGG\n","        self.conv1 = nn.Conv2d(3, 10, kernel_size=5) # 3 канала, 10 ядер, размер ядра - 5\n","        # картинка сожмется на x - (kernel_size - 1)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        # картинка сожмется на x - (kernel_size - 1)\n","        self.pool = nn.MaxPool2d(2) # макс-пулинг: уменьшим на 2\n","        # картинка сожмется в два раза\n","        self.flatten = nn.Flatten()\n","\n","        # head\n","        # исходные картинки были 256х256х3. После первого слоя сверток стало: 252х252х10)\n","        # после второго слоя сверток стало: 248х248х20\n","        # после пулинга стало 124х124х20\n","        self.fc1 = nn.Linear(124 * 124 * 20, 128) # слой на 128 нейронов\n","        self.fc2 = nn.Linear(128, 2) # выходной слой на 2 нейрона (можно было 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.flatten(x)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"vyyjtNhNi6P3","executionInfo":{"status":"ok","timestamp":1697964244822,"user_tz":-180,"elapsed":585,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["torchic = Torchic()\n","optimizer = opt.Adam(torchic.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWgPC1L-i6P3"},"outputs":[],"source":["train(torchic, optimizer, 10)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"u0PEY_Kii6P4","executionInfo":{"status":"ok","timestamp":1697964346851,"user_tz":-180,"elapsed":222,"user":{"displayName":"Alexandra Ivoylova","userId":"03566560446750535462"}}},"outputs":[],"source":["def matplotlib_imshow(img, one_channel=False):\n","    \"\"\"A function for plotting unnormalized images, but it still gets clipping warning\"\"\"\n","    if one_channel:\n","        img = img.mean(dim=0)\n","    img = img * std + mean\n","    npimg = img.numpy()\n","    if one_channel:\n","        plt.imshow(npimg, cmap=\"Greys\")\n","    else:\n","        plt.imshow(np.transpose(npimg, (0, 1, 2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZILGAIbmi6P4"},"outputs":[],"source":["classes = {0: 'Лисена', 1: 'Мороша'}\n","\n","torchic.eval()\n","for i in range(10):\n","    # нам здесь приходится перевернуть, чтобы обратно разнормализовать нашу картинку\n","    matplotlib_imshow(traintest['val'][i][0].permute(1, 2, 0))\n","    plt.show()\n","    print(f\"Dat is {classes[traintest['val'][i][1]]}\")\n","    ypred = torchic(traintest['val'][i][0].unsqueeze(1).permute(1, 0, 2, 3))\n","    print(f\"Torchic thinks it is {classes[torch.argmax(ypred, dim=-1).item()]}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"6eb9e1cf2af2cf6251f1c932a803c6b2f25b1e2cfa2de873853bae064510a498"}},"colab":{"provenance":[{"file_id":"https://github.com/fortvivlan/catset/blob/main/cats.ipynb","timestamp":1697964099189}]}},"nbformat":4,"nbformat_minor":0}