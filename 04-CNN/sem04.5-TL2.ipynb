{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4Vs8v1VwsWx"
   },
   "source": [
    "Данный ноутбук позаимствован с курса по Deep Learning в Сколтехе для магистров!\\\n",
    "Автор: Егор Бурков\n",
    "\n",
    "Также хороший пример transfer learning приведен на [странице](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpVRB4G1V-XT"
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Imagine we want to train a CNN to solve some task B, and we have a CNN trained to solve task A.\n",
    "\n",
    "We can adapt that trained CNN to solve task B. This is known as **transfer learning**. Transfer learning from a complex task A is almost always better than training from scratch for task B; it prevents overfitting and increases accuracy. We will review three methods of transfer learning.\n",
    "\n",
    "The rest of this notebook will guide you through the following:\n",
    "\n",
    "1. Review the target task B (telling apart architectural heritage elements on images).\n",
    "2. Download a CNN that has been pretrained to solve a far more complex task A (1000-class [ImageNet](http://image-net.org/) classification).\n",
    "3. Try classifying CNN's outputs by classical machine learning to solve task B (use CNN as a black box feature extractor).\n",
    "4. Replace CNN's last layers and train only them for task B.\n",
    "5. Train (*fine-tune*) the whole network for task B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xyzy0MJmV-XV"
   },
   "source": [
    "*(service code)* Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24910,
     "status": "ok",
     "timestamp": 1634138853890,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "aCMfMIHLV-XY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip architect.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meuwryMbV-Xr"
   },
   "source": [
    "### 1. Architectural Heritage Elements Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELniTbubV-X5"
   },
   "source": [
    "*(service code)* Split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1634138860213,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "HGiZr4WbV-YD"
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder('data/' + 'train')\n",
    "val_dataset   = torchvision.datasets.ImageFolder('data/' + 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cnNDCrwV-YK"
   },
   "source": [
    "`ImageFolder` is a special case of a very important class `torch.utils.data.Dataset`.\n",
    "\n",
    "Any `Dataset` supports `len()` and indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1634138860214,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "mOlx_OnlV-YL",
    "outputId": "5609e2dc-dba8-40be-b05b-890d2f64b000"
   },
   "outputs": [],
   "source": [
    "print(f\"{len(train_dataset)} training images\")\n",
    "print(f\"{len( val_dataset)} validation images\")\n",
    "\n",
    "image, label = train_dataset[50]\n",
    "print(f\"Dataset returns {type(image)} and {type(label)}\\n\")\n",
    "\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOOAby_UV-YU"
   },
   "source": [
    "Let's explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1634138860214,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "YG0ku7RvV-YV",
    "outputId": "e88860aa-e24c-4e74-f652-72f4952698f1"
   },
   "outputs": [],
   "source": [
    "image, label = train_dataset[8000]\n",
    "print(class_names[label])\n",
    "print(\"Image size:\", image.size)\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8JzJ3O6V-Yc"
   },
   "source": [
    "### 2. CNNs pretrained on the *ImageNet* dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IPd2p1wV-Yd"
   },
   "source": [
    "#### 2.1 The ImageNet challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgnIIR4oV-Yf"
   },
   "source": [
    "In ImageNet, there are ~1M images of 1000 classes.\n",
    "\n",
    "Download the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1634138860215,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "2p99xHcrV-Yg",
    "outputId": "d4222221-3827-48b8-8fbd-1e2f2a286492"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "LABELS_URL = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'\n",
    "with urllib.request.urlopen(LABELS_URL) as f:\n",
    "    imagenet_class_names = json.loads(f.read().decode())\n",
    "\n",
    "print(f\"There are {len(imagenet_class_names)} classes in ImageNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VF1TgFA0V-Yq"
   },
   "source": [
    "Explore those classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1634138860524,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "xrAh69-5V-Yr",
    "outputId": "608acdbb-7f3a-4986-f9bf-f464e27c435d"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "print(\"10 random ImageNet classes:\\n\")\n",
    "for _ in range(10):\n",
    "    class_idx = random.randint(0, len(imagenet_class_names))\n",
    "    print(\"Class %d: %s\" % (class_idx, imagenet_class_names[class_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOTwNFkuV-Yx"
   },
   "source": [
    "#### 2.2 Pretrained CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFCOCM1uV-Yz"
   },
   "source": [
    "`torchvision.models` [provides](https://pytorch.org/docs/stable/torchvision/models.html) pretrained CNNs, including those trained on ImageNet.\n",
    "\n",
    "Let's use for example the \"[SqueezeNet 1.1](https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py)\" CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1634138860525,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "kA7S79BGV-Y5",
    "outputId": "46e267e9-276e-438d-b221-639a28df238e"
   },
   "outputs": [],
   "source": [
    "help(torchvision.models.squeezenet1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOH7Io1OV-Y_"
   },
   "source": [
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-26_at_6.04.32_PM.png\" width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tWE6GoLV-ZB"
   },
   "source": [
    "Download pretrained weights and create the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "c9ab0c2b30fa4efcacf71084b80b93ca",
      "e3808a15258a44dea7a848dcbdf63c9b",
      "847d32b35ae64a54a163627ee87f413a",
      "aed0c32eafc14f1cb7375836c9d5a786",
      "9f70cfe3119341c2b42111fc4b566f76",
      "38c32d561db848ecbc35786db059314d",
      "f24b9d24eea041bba9ea3d87d31192a7",
      "2631fdcdfb7b4af0a1a5fe54f4bbc055",
      "39231bcd21354cb1af85c7c5e5d4833b",
      "8a83f7fff1024fb2846576661d6755c6",
      "513f72268399411590ee646e6c7c4464"
     ]
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1634138860875,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "bbVAe_dQV-ZB",
    "outputId": "e12d487f-4ada-4565-af74-33fb8a1badc2"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.squeezenet1_1(weights='SqueezeNet1_1_Weights.IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1634138860875,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "jZ_gJrI60e7l",
    "outputId": "39d23688-29e7-4c7a-964b-54a195636fe3"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xghN1eZeV-ZH"
   },
   "source": [
    "* If you only want to evaluate the model, call `.eval()`.\n",
    "* If you want to train the model, call `.train()`.\n",
    "\n",
    "This is needed because some layers behave differently during\n",
    "training and testing, for instance dropout or batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1634138860876,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "HPUDsoxGV-ZH"
   },
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akYfF6CkV-ZP"
   },
   "source": [
    "Also, if you don't train (i.e. don't need gradients with respect to parameters), call `.requires_grad_(False)` to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1634138860876,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "85Y9Q72UV-ZR"
   },
   "outputs": [],
   "source": [
    "model.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SghsN6TSV-ZV"
   },
   "source": [
    "First, let's try to forward some garbage through the model.\n",
    "\n",
    "We will have to use images at least of size $224 \\times 224$ pixels, [read in the docs why](https://pytorch.org/vision/stable/models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1634138861225,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "BxbkPSoXV-ZW",
    "outputId": "43dd09f0-a4e4-4a89-b6a4-4a51f5d1a989"
   },
   "outputs": [],
   "source": [
    "# A mini-batch of 5 random images\n",
    "sample_input = torch.randn(5, 3, 224, 224)\n",
    "sample_output = model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-N6cU4eV-Zc"
   },
   "source": [
    "The output is class scores (not probabilities yet).\n",
    "\n",
    "The higher the $i$-th score, the higher the CNN's confidence that this image is of $i$-th class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1634138861226,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "MBNePU1pV-Ze",
    "outputId": "90331d42-90ce-4665-f232-01787f4088a9"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The CNN has returned a tensor of {sample_output.shape} \"\n",
    "    f\"with values from {sample_output.min().item()} to {sample_output.max().item()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGGLViapV-Zj"
   },
   "source": [
    "Now time to forward some real images through the CNN.\n",
    "\n",
    "Before that, they have to be normalized [as per the documentation](https://pytorch.org/vision/stable/models.html):\n",
    "\n",
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. You can use the following transform to normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1634138861226,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "adJNuGzDV-Zj"
   },
   "outputs": [],
   "source": [
    "normalize = torchvision.transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1634138861227,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "U5VOcQj-V-Zo"
   },
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "def predict(image):\n",
    "    \"\"\"\n",
    "    image:\n",
    "        PIL.Image\n",
    "\n",
    "    return:\n",
    "        tensor, shape == (1000,)\n",
    "        ImageNet class probabilities.\n",
    "    \"\"\"\n",
    "    # Scale the image to 224x224 pixels\n",
    "    print('before resize:', image.size)\n",
    "    image = image.resize((224, 224))\n",
    "    print('after resize:', image.size)\n",
    "\n",
    "    # Convert `PIL.Image` to `torch.tensor` and normalize\n",
    "    image = torchvision.transforms.ToTensor()(image)\n",
    "    image = normalize(image)\n",
    "    \n",
    "    # Add singleton batch dimension with [None]\n",
    "    image = image[None]\n",
    "    \n",
    "    # Predict class scores\n",
    "    prediction = model(image)\n",
    "    \n",
    "    # Convert class scores to probabilities\n",
    "    prediction = prediction.softmax(axis = 1)\n",
    "    \n",
    "    # Remove singleton batch dimension\n",
    "    \n",
    "    return prediction[0],prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHC1Xwb8V-Zs"
   },
   "source": [
    "Try to classify some real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1634138861988,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "5qO3SzqLV-Zs",
    "outputId": "14c4276d-2795-4f7f-baa9-23591c8e957e"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "# IMAGE_URL = \"http://qph.fs.quoracdn.net/main-qimg-3c711f9ad560f1703125f2839f5d8ad6\"\n",
    "# IMAGE_URL = \"http://bestfunnies.com/wp-content/uploads/2015/05/TOP-30-Cute-Cats-Cute-Cat-2-570x428.jpg\"\n",
    "# IMAGE_URL = \"https://i.pinimg.com/originals/23/dd/90/23dd903fc557cfbc95f0176c24829c95.jpg\"\n",
    "IMAGE_URL = \"https://i.pinimg.com/474x/6e/d3/0c/6ed30c47246083096f0d9b79e122dace.jpg\"\n",
    "\n",
    "with urllib.request.urlopen(IMAGE_URL) as f:\n",
    "    image = PIL.Image.open(f)\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1634138861989,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "1dQXbzQU33H2",
    "outputId": "874e3731-4eb4-4425-b7fa-e7078a123b5f"
   },
   "outputs": [],
   "source": [
    "image.resize((224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1634138862461,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "Lhmv8mYsV-Zx",
    "outputId": "01d7cf9d-259e-407d-ebf2-ded09b53fd99"
   },
   "outputs": [],
   "source": [
    "probabilities,proba = predict(image)\n",
    "\n",
    "top_probabilities, top_indices = torch.topk(probabilities, 10)\n",
    "print(\"10 most probable classes are:\")\n",
    "for probability, class_idx in zip(top_probabilities, top_indices):\n",
    "    print(\"%.4f: %s\" % (probability, imagenet_class_names[class_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B73IxZmEV-Z1"
   },
   "source": [
    "### 3. Use classical machine learning with a pretrained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPGb6yVHV-Z2"
   },
   "source": [
    "Our network is some convolutional part `model.features` followed by a classification block `model.classifier` (you learn this by reading model's `forward()` in its [source code](https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "linAKAnwV-Z3"
   },
   "source": [
    "`model.features` outputs a stack of 512 uninterpretable feature maps, of size $20 \\times 20$ in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1634138862462,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "mUDj46MxV-Z7",
    "outputId": "a5597633-4611-4782-cc63-85ef9ab0c51d"
   },
   "outputs": [],
   "source": [
    "sample_input = torch.randn(5, 3, 333, 333)\n",
    "model.features(sample_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dxV9LRWV-aB"
   },
   "source": [
    "Which are then passed through this `model.classifier` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1634138862462,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "I8KPDCPuV-aB",
    "outputId": "e29db55e-3fc1-48e5-cf77-eb980b35f0db"
   },
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBq0p3-7V-aF"
   },
   "source": [
    "Although the outputs of `model.features` are uninterpretable, they contain a lot of useful information about input images. They are called *latent encodings* or *latent embeddings* of the input images, or *intermediate features* (hence the name `model.features`).\n",
    "\n",
    "Here is an example visualization of these features ([source](https://www.researchgate.net/figure/t-SNE-embedding-of-a-subset-of-the-convolutional-features-extracted-from-Imagenet_fig1_283762714)):\n",
    "\n",
    "![](https://www.researchgate.net/profile/Zhongfei_Zhang/publication/283762714/figure/fig1/AS:614293271756808@1523470337103/t-SNE-embedding-of-a-subset-of-the-convolutional-features-extracted-from-Imagenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z49M_TqLV-aG"
   },
   "source": [
    "These features can be sufficient to recognize architectural elements in images. Let's train a random forest classifier on these features.\n",
    "\n",
    "Sometimes it's called \"using a CNN as a *black-box feature extractor*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8-V-y8HV-aH"
   },
   "source": [
    "First, implement image preprocessing.\n",
    "\n",
    "Images have to be of same size ($224 \\times 224$ in our case) for efficiency: we will stack them in tensors of size $B \\times 3 \\times 224 \\times 224$ representing mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1634138862463,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "ZzbxRscqV-aM",
    "outputId": "0f9a41b8-7b16-48ce-f04e-9ee4619a72ef"
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224,224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "train_dataset_224 = torchvision.datasets.ImageFolder('data/' + 'train', transform)\n",
    "val_dataset_224   = torchvision.datasets.ImageFolder('data/'+'test'  , transform)\n",
    "\n",
    "image, _ = train_dataset_224[66]\n",
    "print(f\"Datasets now return images as tensors of size {image.shape} and datatype {image.dtype}\")\n",
    "\n",
    "assert \\\n",
    "    torch.is_tensor(image) and \\\n",
    "    image.dtype == torch.float32 and \\\n",
    "    image.shape == (3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ5m0muqV-aY"
   },
   "source": [
    "For even better efficiency, we use `torch.utils.data.DataLoader` class. It is a wrapper around `torch.utils.data.Dataset` that automatically loads data in the background and groups them into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1634138862463,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "zz_g6mLIV-aZ"
   },
   "outputs": [],
   "source": [
    "??torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1634138862507,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "wxp4-vWvV-ac",
    "outputId": "9161b1b5-a155-400f-8bf7-3c68a4f96015"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset_224, batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset_224, batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"Train/val dataloaders have {len(train_dataloader)} and {len(val_dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWs23LHzV-ae"
   },
   "source": [
    "Upload the model to GPU (if it's available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1634138862508,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "jdpuva4tV-af"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14520,
     "status": "ok",
     "timestamp": 1634138876978,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "dD7R47ltV-ai"
   },
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "\n",
    "assert str(list(model.parameters())[0].device) == DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsus-fuxV-al"
   },
   "source": [
    "Compute embeddings for the whole dataset.\n",
    "\n",
    "Also, let's average the features over spatial dimensions (i.e. apply *global average pooling*). We do this because we don't care in which part of an image a cat/dog was, but rather \"how much of a cat/dog\" there was.\n",
    "\n",
    "This way we encode every image with a **512-dimensional vector**. (in the image below, $d$ could be 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcwGgljwV-am"
   },
   "source": [
    "![image](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f2vvrTz6XSC"
   },
   "source": [
    "Below let us run `features` part of our net in order to extract latent embeddings. Next we avearge outputs over spatial dimensions and end up one dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1634138876981,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "yCGtJ9VcV-an"
   },
   "outputs": [],
   "source": [
    "def compute_embeddings(dataloader):\n",
    "    \"\"\"\n",
    "    Compute latent embeddings for all images in `dataloader` using `model` CNN.\n",
    "    \n",
    "    dataloader:\n",
    "        torch.utils.data.DataLoader\n",
    "    \n",
    "    return:\n",
    "    X:\n",
    "        tensor, shape: (len(dataloader.dataset), 512), dtype: float32, device: 'cpu'\n",
    "    y:\n",
    "        tensor, shape: (len(dataloader.dataset),)    , dtype: int64  , device: 'cpu'\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            y.append(labels)\n",
    "\n",
    "            features = model.features(images.to(DEVICE)).mean([2, 3]) # (B, 512, 13, 13)\n",
    "            print(features.shape)\n",
    "            X.append(features.cpu())\n",
    "\n",
    "    return torch.cat(X), torch.cat(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1634138877690,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "GBWtESDT6_vd",
    "outputId": "81c0347e-6076-43ff-bf32-ae4953bc4875"
   },
   "outputs": [],
   "source": [
    "images, _ = iter(train_dataloader).next()\n",
    "features = model.features(images.to(DEVICE)).mean([2, 3]) # (B, 512, 13, 13)\n",
    "print(features.shape)  # the shape of tensor representing 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32171,
     "status": "ok",
     "timestamp": 1634138909856,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "0SPbMfLXV-aq",
    "outputId": "e701e167-6c72-4016-db81-ed57c25fbbe3"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = compute_embeddings(train_dataloader)\n",
    "X_val  , y_val   = compute_embeddings(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1634138909856,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "N2JF3FKh78M-"
   },
   "outputs": [],
   "source": [
    "assert X_train.shape == (len(train_dataloader.dataset), 512)\n",
    "assert y_train.shape == (len(train_dataloader.dataset),)\n",
    "assert X_val.shape   == (len(val_dataloader.dataset), 512)\n",
    "assert y_val.shape   == (len(val_dataloader.dataset),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1634138909857,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "AZHfw6hf57oK",
    "outputId": "4c301be9-5249-4424-e8ac-6475813ffe2a"
   },
   "outputs": [],
   "source": [
    "len(train_dataloader.dataset), len(val_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-NeDAXHV-as"
   },
   "source": [
    "Train the random forest classifier on the embeddings that we just precomputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19220,
     "status": "ok",
     "timestamp": 1634138929072,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "FA8sNGxQV-at",
    "outputId": "b07e07aa-2ba2-4cc1-9189-26d118676779"
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "\n",
    "classifier = sklearn.ensemble.RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1634138929552,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "cY-K57yvV-av",
    "outputId": "53dc265f-832b-4f4e-dc7d-b13dfe6f98d2"
   },
   "outputs": [],
   "source": [
    "train_accuracy = sklearn.metrics.accuracy_score(y_train, classifier.predict(X_train))\n",
    "val_accuracy   = sklearn.metrics.accuracy_score(y_val  , classifier.predict(X_val))\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G19aIo67V-ay"
   },
   "source": [
    "### 4. Replace the last layers and retrain them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_y8IzdtV-az"
   },
   "source": [
    "The above approach isn't very convenient because we have to run a separate algorithm.\n",
    "\n",
    "It's more convenient to add a neural network on top of `model.CNN` instead of a random forest. This way, our pipeline becomes **end-to-end**: a single `torch.nn.Module` directly outputs the prediction we need (10 class scores).\n",
    "\n",
    "Still, don't expect the accuracy to be much higher than with random forest. We just replaced it with a simple two-layer neural net, nothing more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1634138929553,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "sfMSMHqvV-a0"
   },
   "outputs": [],
   "source": [
    "class RetrofittedCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        pretrained_net = torchvision.models.squeezenet1_1(weights='SqueezeNet1_1_Weights.IMAGENET1K_V1')\n",
    "        self.CNN = pretrained_net.features\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(512, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(512, len(class_names))\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Define the forward pass.\n",
    "        \"\"\"\n",
    "        features = self.CNN(images).mean([2, 3])\n",
    "        return self.classifier(features)\n",
    "\n",
    "torch.manual_seed(666) # for reproducibility\n",
    "model = RetrofittedCNN().to(DEVICE)\n",
    "model.CNN.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MT9JAfhhV-a2"
   },
   "source": [
    "Set the optimization parameters.\n",
    "\n",
    "Note that we only want to train the classifier!\n",
    "\n",
    "**Importnat:** here we freeze weights of `CNN` part in order not to train its weihgts. It is achieved by passing only needed part weights to an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1634138930027,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "gDYfeV5UV-a3"
   },
   "outputs": [],
   "source": [
    "trainable_parameters = model.classifier.parameters()\n",
    "\n",
    "learning_rate = 3e-3\n",
    "optimizer = torch.optim.Adam(\n",
    "    trainable_parameters, lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Our loss function is negative log likelihood of the correct class\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gzRURp4V-a7"
   },
   "source": [
    "Describe the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1634138935681,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "HvwE6xdVV-a8"
   },
   "outputs": [],
   "source": [
    "# Please read this cell and understand what's going on!\n",
    "\n",
    "def validate(model, dataloader):\n",
    "    \"\"\"Compute accuracy on the `dataloader` dataset.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            labels = labels.to(DEVICE)\n",
    "            probabilities = model(images.to(DEVICE))\n",
    "            predictions = probabilities.max(1)[1]\n",
    "\n",
    "            total += len(labels)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            \n",
    "    return correct / total\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    \"\"\"Train for one epoch, return accuracy and average loss.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(dataloader):\n",
    "        probabilities = model(images.to(DEVICE))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            labels = labels.to(DEVICE)\n",
    "            predictions = probabilities.max(1)[1]\n",
    "            \n",
    "            total += len(labels)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "\n",
    "        loss_value = criterion(probabilities, labels)\n",
    "        total_loss += loss_value.item() * len(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return correct / total, total_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1634138937695,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "qiC5V4AlV-a-"
   },
   "outputs": [],
   "source": [
    "def set_learning_rate(optimizer, new_learning_rate):\n",
    "    \"\"\"Set learning rates of the optimizer to `new_learning_rate`.\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1634138939676,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "KA52hmG1V-bB"
   },
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "train_accuracy, val_accuracy, train_loss = float('nan'), float('nan'), float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1634138943111,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "TylHvJJG8jqa",
    "outputId": "d193cb7a-b902-4305-8712-68ba1e48dd6b"
   },
   "outputs": [],
   "source": [
    "images, labels = iter(train_dataloader).next()\n",
    "probabilities = model(images.to(DEVICE))\n",
    "print(probabilities.shape)\n",
    "predictions = probabilities.max(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 820,
     "referenced_widgets": [
      "98d8573d03ba418ea2db52cf1273f46f",
      "5dd0ce1d4d5d46d89fbc78af192846b1",
      "59b15400b9bc401fbf4b45dde6795b10",
      "6427ea10908043658b5aaa8e62aef2c8",
      "0248d7ab86804dd8b71c790702e8f633",
      "35f7ee467dae4b419156937db5b7c6ba",
      "036ffeee9f53493582aff4dfdde959d4",
      "bfb3fbf5e5d340129e64f2f73a9fbec3",
      "2e6c5c3f15ca4b5bbe1c25b445a3a4e9",
      "800c386de8ce419aad212c6f1df341c6",
      "f557c5a626724a79ad864cea65831d52",
      "9e9cd17e96b54dca91f720e8597d7fbb",
      "1241be1846ab47a1b2769b49eeeef802",
      "2557218e3f914e42abcb932751ab9040",
      "d23f96bf21d84fecbd65510ae93b9a6b"
     ]
    },
    "executionInfo": {
     "elapsed": 480862,
     "status": "ok",
     "timestamp": 1634077211926,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "HpEChH0rV-bI",
    "outputId": "1b2d0360-20fe-4ee3-f9de-032fd2ceb57f"
   },
   "outputs": [],
   "source": [
    "for _ in range(15):\n",
    "    # Compute validation accuracy\n",
    "    val_accuracy = validate(model, val_dataloader)\n",
    "    print(\n",
    "        f\"After {epochs} epochs, training accuracy: {train_accuracy * 100:.2f}%\"\n",
    "        f\" (loss {train_loss:.4f}), validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Train for one epoch\n",
    "    train_accuracy, train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "    epochs += 1\n",
    "\n",
    "    # Decrease learning rate sometimes\n",
    "    if epochs in (8, 12, 14):\n",
    "        learning_rate = learning_rate/10\n",
    "        set_learning_rate(optimizer, learning_rate)\n",
    "        print(f\"Decreasing the learning rate to {learning_rate}\")\n",
    "\n",
    "# Compute final validation accuracy\n",
    "val_accuracy = validate(model, val_dataloader)\n",
    "print(\n",
    "    f\"After {epochs} epochs, training accuracy: {train_accuracy * 100:.2f}%\"\n",
    "    f\" (loss {train_loss:.4f}), validation accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD52GOx-BOCg"
   },
   "source": [
    "It looks like the neural network classifier generalizes better than random forest. Anyway, they all are limited by the \"frozen\" CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knrXXJbIV-bL"
   },
   "source": [
    "### 5. Fine-tune the whole network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKsZwF1mV-bM"
   },
   "source": [
    "We have seen that `model.features` is already a good feature extractor. Still, it's not aware of our dataset.\n",
    "\n",
    "Let's train **all** layers (including our new retrofitted classifier) to get extra accuracy. This process is called **fine-tuning**.\n",
    "\n",
    "Because the original `model.features`' weights are valuable, fragile and co-adapted, fine-tuning requires several precautions:\n",
    "\n",
    "* If the retrofitted classifier's weights are random, it will propagate large gradients into the pre-trained CNN. That can wreck the pre-trained weights. That's why one should either\n",
    "  * train the retrofitted classifier alone for a while (**we've just done this**); or\n",
    "  * assign a larger learning rate to the classifier and a smaller one to pre-trained weights (e.g. $10^{-2}$ and $10^{-4}$ with SGD); or\n",
    "  * (an option for the lazy) train everything with a reduced learning rate (e.g. $10^{-3}$).\n",
    "* Use a less aggressive optimizer, e.g. momentum SGD, Adam with warm-up (**we do this below**), or RAdam.\n",
    "* Use a smaller learning rate than usually. **Below I used $\\leq 10^{-4}$**, while on random weights I'd use something around $10^{-3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1634138950675,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "ds4bMGGr_X5U"
   },
   "outputs": [],
   "source": [
    "# Don't forget to turn gradient computing back on.\n",
    "# We \"froze\" the CNN above. \"Unfreeze\" it.\n",
    "model.CNN.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ejfzn-n_WWC"
   },
   "source": [
    "Here, we set zero learning rate to warm Adam up.\n",
    "\n",
    "Warm-up means running some iterations without any weight updates, just to let Adam accumulate gradient statistics. Otherwise, its first optimization steps will be very noisy and destructive for the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1634139011890,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "SjET1PwvV-bR"
   },
   "outputs": [],
   "source": [
    "# Now the parameters have to include both CNN's and classifier's weights\n",
    "trainable_parameters = model.parameters()\n",
    "\n",
    "learning_rate = 0\n",
    "optimizer = torch.optim.Adam(\n",
    "    trainable_parameters, lr=learning_rate, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpotA1JWARSM"
   },
   "source": [
    "Run 1 epoch of warm-up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2e66d185a0a14da6ba3ae733a22418d1",
      "f383bbbf980848deb0b9e4c26af415b6",
      "0ac236d224bd43f4a02736a2c65760f3",
      "745ec31f487f4eeda7cf98f703ea8f4e",
      "8ecf620a14404a4c90dfda1ea965524f",
      "6fe513a0b20743c4b7c54f51b0092edb",
      "55e3e4aad7394805a18165b3593be29b",
      "5c7bda4f9c904a1f946e3e0b5538fd8f",
      "8b62f6371fe544889a3406609ca03eb6",
      "14e4038982e8441dbfe937e03be5d3bc",
      "19d912a2243f482490bd7fe7da7f0714"
     ]
    },
    "executionInfo": {
     "elapsed": 38147,
     "status": "ok",
     "timestamp": 1634139065684,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "7eBHgkhGvVc7",
    "outputId": "8a600bff-c572-4e33-bba4-57359d10ddb7"
   },
   "outputs": [],
   "source": [
    "train(model, train_dataloader, criterion, optimizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNi9vgfaAYzh"
   },
   "source": [
    "Switch back to a positive learning rate. Remember, it should be smaller than usual (above, we used $3 \\cdot 10^{-3}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1634139066532,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "nKcrlYGh5n8r"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "set_learning_rate(optimizer, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16LLTW_vAxSo"
   },
   "source": [
    "Finally, fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1634139077176,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "zEauyylhV-bV"
   },
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "train_accuracy, val_accuracy, train_loss = float('nan'), float('nan'), float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803,
     "referenced_widgets": [
      "1132245b93ed4af0b5420e331a80ef80",
      "b4d4618a86204713a01f492ba6e04337",
      "12f3ef64594049c38f953fb9c3fa8e8c",
      "9b411f8607a04030a3f100613a660cc9",
      "f6780d6ef80b4f9d8f80d070e6c28697",
      "e970c2f4d30043f38da28c8651edad53",
      "9b693ccb8aac4fc28c06ee21df19535b",
      "cb03c14f95154989964ad6743e761118",
      "f2f796c1ac3945c3ba6f97cb9321d48f",
      "b07bac9dcd1f4124b12a1169c7b9c568",
      "68b796d7deb042ed9872f533312f2887",
      "382204110c954766a9df7c822ee4eafa",
      "ae86fa79a7f344b88c62df76cd2456d4",
      "eb3832f4308448cbbcc7610af779d86a",
      "404134a97ea643818f27824908d9e0bd",
      "08f9ebcfbcb34630b64678a89346ebdc",
      "bfa1670615ce47ccbaa8101bac45c216",
      "043fd3ae83ab4e21aaa9a0adec408728",
      "fc263514155849c5ae7f67ec4616ab0d",
      "ea7b7203fc9a43448b358656c390b24a",
      "a3f4114f73cc40c2b79c2397317ccd76",
      "3ded0627b339466e9968146d494561c5",
      "5c18fdf1c5b449d0925c54d48067b1ae",
      "3a1a14c45adf4e8ea12108b5db4cdb9d",
      "cb423781d13843baa0e737f41ecb7cb3"
     ]
    },
    "executionInfo": {
     "elapsed": 622367,
     "status": "ok",
     "timestamp": 1634103859796,
     "user": {
      "displayName": "иван иванов",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02926776193083975428"
     },
     "user_tz": -180
    },
    "id": "V_j2w6NGV-bZ",
    "outputId": "03353a0d-7451-4944-aa2c-74ac1edf24cd"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Copy the training loop from above.\n",
    "# - Train for 15 epochs.\n",
    "# - Drop learning rate by a factor of 10 at epochs 7 and 12.\n",
    "\n",
    "epoch_number = 15\n",
    "for _ in range(epoch_number):\n",
    "    # Compute validation accuracy\n",
    "    val_accuracy = validate(model, val_dataloader)\n",
    "    print(\n",
    "        f\"After {epochs} epochs, training accuracy: {train_accuracy * 100:.2f}%\"\n",
    "        f\" (loss {train_loss:.4f}), validation accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Train for one epoch\n",
    "    train_accuracy, train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "    epochs += 1\n",
    "\n",
    "    # Decrease learning rate sometimes\n",
    "    if epochs in (7, 12):\n",
    "        learning_rate /= 10\n",
    "        set_learning_rate(optimizer, learning_rate)\n",
    "        print(f\"Decreasing the learning rate to {learning_rate}\")\n",
    "\n",
    "# Compute final validation accuracy\n",
    "val_accuracy = validate(model, val_dataloader)\n",
    "print(\n",
    "    f\"After {epochs} epochs, training accuracy: {train_accuracy * 100:.2f}%\"\n",
    "    f\" (loss {train_loss:.4f}), validation accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYraka9qRyyV"
   },
   "outputs": [],
   "source": [
    "epoch_number = 15\n",
    "torch.save(model.state_dict(), f'weights_{epoch_number}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ver6OXgGkm8c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transfer_Learning_PyTorch.ipynb",
   "provenance": [
    {
     "file_id": "1P9hA1UQtwSFX6_1oD2av4kN12SEOIcz3",
     "timestamp": 1634072785525
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ac236d224bd43f4a02736a2c65760f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55e3e4aad7394805a18165b3593be29b",
      "placeholder": "​",
      "style": "IPY_MODEL_6fe513a0b20743c4b7c54f51b0092edb",
      "value": "100%"
     }
    },
    "1132245b93ed4af0b5420e331a80ef80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_12f3ef64594049c38f953fb9c3fa8e8c",
       "IPY_MODEL_9b411f8607a04030a3f100613a660cc9",
       "IPY_MODEL_f6780d6ef80b4f9d8f80d070e6c28697"
      ],
      "layout": "IPY_MODEL_b4d4618a86204713a01f492ba6e04337"
     }
    },
    "12f3ef64594049c38f953fb9c3fa8e8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b693ccb8aac4fc28c06ee21df19535b",
      "placeholder": "​",
      "style": "IPY_MODEL_e970c2f4d30043f38da28c8651edad53",
      "value": "100%"
     }
    },
    "14e4038982e8441dbfe937e03be5d3bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19d912a2243f482490bd7fe7da7f0714": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2631fdcdfb7b4af0a1a5fe54f4bbc055": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2e66d185a0a14da6ba3ae733a22418d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ac236d224bd43f4a02736a2c65760f3",
       "IPY_MODEL_745ec31f487f4eeda7cf98f703ea8f4e",
       "IPY_MODEL_8ecf620a14404a4c90dfda1ea965524f"
      ],
      "layout": "IPY_MODEL_f383bbbf980848deb0b9e4c26af415b6"
     }
    },
    "38c32d561db848ecbc35786db059314d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39231bcd21354cb1af85c7c5e5d4833b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "513f72268399411590ee646e6c7c4464": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55e3e4aad7394805a18165b3593be29b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c7bda4f9c904a1f946e3e0b5538fd8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68b796d7deb042ed9872f533312f2887": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fe513a0b20743c4b7c54f51b0092edb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "745ec31f487f4eeda7cf98f703ea8f4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b62f6371fe544889a3406609ca03eb6",
      "max": 377,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5c7bda4f9c904a1f946e3e0b5538fd8f",
      "value": 377
     }
    },
    "847d32b35ae64a54a163627ee87f413a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f24b9d24eea041bba9ea3d87d31192a7",
      "placeholder": "​",
      "style": "IPY_MODEL_38c32d561db848ecbc35786db059314d",
      "value": "100%"
     }
    },
    "8a83f7fff1024fb2846576661d6755c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b62f6371fe544889a3406609ca03eb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ecf620a14404a4c90dfda1ea965524f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19d912a2243f482490bd7fe7da7f0714",
      "placeholder": "​",
      "style": "IPY_MODEL_14e4038982e8441dbfe937e03be5d3bc",
      "value": " 377/377 [00:37&lt;00:00, 10.73it/s]"
     }
    },
    "9b411f8607a04030a3f100613a660cc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2f796c1ac3945c3ba6f97cb9321d48f",
      "max": 377,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb03c14f95154989964ad6743e761118",
      "value": 377
     }
    },
    "9b693ccb8aac4fc28c06ee21df19535b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f70cfe3119341c2b42111fc4b566f76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_513f72268399411590ee646e6c7c4464",
      "placeholder": "​",
      "style": "IPY_MODEL_8a83f7fff1024fb2846576661d6755c6",
      "value": " 4.73M/4.73M [00:00&lt;00:00, 12.0MB/s]"
     }
    },
    "aed0c32eafc14f1cb7375836c9d5a786": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39231bcd21354cb1af85c7c5e5d4833b",
      "max": 4958839,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2631fdcdfb7b4af0a1a5fe54f4bbc055",
      "value": 4958839
     }
    },
    "b07bac9dcd1f4124b12a1169c7b9c568": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4d4618a86204713a01f492ba6e04337": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9ab0c2b30fa4efcacf71084b80b93ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_847d32b35ae64a54a163627ee87f413a",
       "IPY_MODEL_aed0c32eafc14f1cb7375836c9d5a786",
       "IPY_MODEL_9f70cfe3119341c2b42111fc4b566f76"
      ],
      "layout": "IPY_MODEL_e3808a15258a44dea7a848dcbdf63c9b"
     }
    },
    "cb03c14f95154989964ad6743e761118": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e3808a15258a44dea7a848dcbdf63c9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e970c2f4d30043f38da28c8651edad53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f24b9d24eea041bba9ea3d87d31192a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2f796c1ac3945c3ba6f97cb9321d48f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f383bbbf980848deb0b9e4c26af415b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6780d6ef80b4f9d8f80d070e6c28697": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68b796d7deb042ed9872f533312f2887",
      "placeholder": "​",
      "style": "IPY_MODEL_b07bac9dcd1f4124b12a1169c7b9c568",
      "value": " 377/377 [00:38&lt;00:00, 10.60it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
