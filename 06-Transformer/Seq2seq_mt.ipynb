{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Машинный перевод на Seq2Seq с использованием torchtext\n","\n","Один из самых известных параллельных датасетов для машинного перевода - [Tatoeba](https://tatoeba.org/en/). В пригодном для обучения виде его можно найти [тут](https://www.manythings.org/anki/). Я для удобства сразу разделила файл на трейн и тест (можно это самостоятельно сделать в колабе):"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!wget -qq http://www.manythings.org/anki/rus-eng.zip \n","!unzip rus-eng.zip\n","\n","with open('rus.txt') as f:\n","    lines = f.readlines() \n","\n","eighty = int(len(lines) * 0.8)\n","\n","with open('train.txt', 'w') as f:\n","    for line in lines[:eighty]:\n","        f.write(line)\n","\n","with open('test.txt', 'w') as f:\n","    for line in lines[eighty:]:\n","        f.write(line)"]},{"cell_type":"markdown","metadata":{},"source":["Для того, чтобы удобным образом делать датасеты и даталоудеры в торче из текстов, существует библиотека torchtext. В ее ранних версиях (примерно до 0.6) вы можете видеть такие классы, как Field, Example, Dataset, BucketIterator - они все были выпилены из более поздних версий, так что если видите какой-нибудь туториал с этими классами, устанавливайте torchtext==0.10.0 или старше. \n","\n","В современных версиях разрабы торча, как обычно, полностью поломали всю обратную совместимость, какая и была, теперь у нас совсем поменялись принципы работы с текстовыми данными. \n","\n","Для того, чтобы обучить сеточку переводить, нам нужно будет сделать такой датасет, который будет плеваться парами предложений source-target, при этом они должны быть токенизированы и добиты паддингом. И не забудем, что нам нужен словарь: для того, чтобы эти самые токены превратить в индексы. \n","\n","Для создания словаря и для загрузки такого датасета с парами предложений у torchtext есть свои инструменты. Заимпортим их. Нам также понадобится spacy для токенизации, для него еще нужно загрузить соответствующие модельки:"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer          # будет брать токенизатор из спейси и пихать его в collate_fn\n","import torchdata.datapipes as dp                        # будет делать итератор из нашего текстового файла\n","from torchtext.vocab import build_vocab_from_iterator   # будет делать словарь\n","from torch.nn.utils.rnn import pad_sequence             # будет падить\n","from torch.utils.data import DataLoader                 # будет грузить батчи\n","import torch.nn.functional as F\n","import spacy \n","\n","# заодно импортнем все нужное для модельки\n","from torch import Tensor\n","import torch\n","import torch.nn as nn\n","from torch.nn import Transformer\n","import math\n","\n","# это просто время засекать\n","from timeit import default_timer as timer\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python -m spacy download en_core_web_sm\n","!python -m spacy download ru_core_news_sm"]},{"cell_type":"markdown","metadata":{},"source":["Потом нам надо будет задать некоторые важные вещи, которые мы будем использовать, и подготовить себе инструменты для токенизации и собственно словари (их по два - ведь у нас два языка. )"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["f:\\CODE\\Python\\Interpreters\\Python310\\lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n","  _C._set_default_tensor_type(t)\n"]}],"source":["# Зададим языки\n","SRC_LANGUAGE = 'en'\n","TGT_LANGUAGE = 'ru'\n","\n","token_transform = {}\n","vocab_transform = {}\n","\n","token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n","token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='ru_core_news_sm')\n","\n","# Зададим спецсимволы, чтобы потом их раскодировывать\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","# Не перепутайте порядок - они в таком порядке и добавятся в словарь\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"]},{"cell_type":"markdown","metadata":{},"source":["Теперь подготовим итератор для датасета и соберем словарь. "]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["def getTokens(data_iter, place):\n","    \"\"\"\n","    Возвращаем списки токенов для соответствующих языков: \n","    наш итератор содержит пары язык-предложение, поэтому\n","    place=0 - это язык-источник, \n","    place=1 - это язык-таргет.\n","    \"\"\"\n","    for english, russian in data_iter:\n","        if place == 0:\n","            yield token_transform['en'](english)\n","        else:\n","            yield token_transform['ru'](russian)\n","\n","def create_iter(path):\n","    \"\"\"\n","    Создаем итератор по пути файла\n","    \"\"\"\n","    pipe = dp.iter.IterableWrapper([path]) # эта штука нужна, чтобы не загружать содержимое файла целиком\n","    pipe = dp.iter.FileOpener(pipe, mode='rb') # приготавливаемся к чтению\n","    pipe = pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True) # парсим файл как тсв (у нас там действительно тсв-формат)\n","    pipe = pipe.map(lambda row: row[:2]) # оставляем только первые две ячейки, а то там дальше лицензия еще\n","    return pipe\n","\n","# Создаем итераторы трейна и валидации. Поскольку у итератора нет длины, а нам она может понадобиться, зададим ее вручную\n","# (я захардкодила, это, конечно, так себе стратегия)\n","train = create_iter('train.txt').set_length(383378)\n","test = create_iter('test.txt').set_length(95845)"]},{"cell_type":"markdown","metadata":{},"source":["Проверим, что все ок:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["next(iter(train))"]},{"cell_type":"markdown","metadata":{},"source":["Собственно словарь собирается с помощью функции build_vocab_from_iterator:"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["vocab_transform['en'] = build_vocab_from_iterator(\n","    getTokens(train, 0), # вызываем нашу функцию токенизации\n","    min_freq=2, # отсеиваем слишком редкие слова\n","    specials= ['<pad>', '<sos>', '<eos>', '<unk>'], # учитываем, что у нас 4 спецсимвола\n","    special_first=True) # и что они идут в начале словаря\n","vocab_transform['ru'] = build_vocab_from_iterator(\n","    getTokens(train, 1), \n","    min_freq=2, \n","    specials= ['<pad>', '<sos>', '<eos>', '<unk>'], \n","    special_first=True)\n","# дальше нужно еще установить дефолтный индекс для OOV-слов\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    vocab_transform[ln].set_default_index(UNK_IDX)"]},{"cell_type":"markdown","metadata":{},"source":["Также нам понадобится маскировать наши предложения: трансформер не должен видеть правый контекст, он же генерирует новый текст, к тому же, маскировать нужно и паддинги, чтобы они не учитывались при оценке:\n","\n","<img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638824585791/vkXCmdGyw.png?auto=compress,format&format=webp\" width=500>"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["def generate_square_subsequent_mask(sz):\n","    \"\"\"\n","    Генерирует маску: она выглядит как матричка-треугольник, где один угол заполнен бесконечностями. \n","    \"\"\"\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","\n","def create_mask(src, tgt):\n","    \"\"\" \n","    Эта функция использует ту, что мы задефайнили выше\n","    \"\"\"\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    # заполняем верхний уголок\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len) \n","    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n","\n","    # учитываем паддинги\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1) \n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"]},{"cell_type":"markdown","metadata":{},"source":["Не забудем, что нам необходимо еще позаботиться о функции collate_fn. "]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["# просто вспомогательная функция-декоратор, чтобы применять другие функции сразу на много элементов\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# добавляет BOS/EOS и создает вектор с индексами инпутов\n","def tensor_transform(token_ids):\n","    return torch.cat((torch.tensor([BOS_IDX]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([EOS_IDX])))\n","\n","# трансформации для ``src`` и ``tgt``, которые собственно превращают наши сырые строки в тензора с индексами\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transforms(token_transform[ln], #токенизация\n","                                               vocab_transform[ln], #индексация токенов\n","                                               tensor_transform) # добавили BOS/EOS и создали тензор\n","\n","\n","# наконец collate_fn\n","def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch: # scr_sample & tgt_sample - это две строки\n","        # отстрипим \\n и прогоним через выше написанные функции\n","        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n","\n","    # отпадим\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return src_batch, tgt_batch"]},{"cell_type":"markdown","metadata":{},"source":["Подготовим себе даталоудеры. "]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["BATCH_SIZE = 256\n","\n","train_iter = DataLoader(train, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","test_iter = DataLoader(test, batch_size=BATCH_SIZE, collate_fn=collate_fn)"]},{"cell_type":"markdown","metadata":{"id":"8FYJe2CA8GcY"},"source":["#### Seq2seq модель\n","\n","Пора писать простой seq2seq. Разобьем модель на несколько модулей - Encoder, Decoder и их объединение. \n","\n","Encoder должен быть подобен символьной сеточке в POS tagging'е: эмбеддить токены и запускать rnn'ку (в данном случае будем пользоваться GRU) и отдавать последнее скрытое состояние.\n","\n","Decoder почти такой же, только еще и предсказывает токены на каждом своем шаге."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"ySJ4tUAqvFvB"},"outputs":[],"source":["batch = next(iter(train_iter)) # это нам для тестирования"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"x8ndCRZLl4ZZ"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=256, num_layers=1):\n","        super().__init__()\n","        \n","        self._num_layers = num_layers\n","        self._hidden_dim = rnn_hidden_dim\n","        \n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.GRU(input_size=emb_dim, hidden_size=self._hidden_dim, \n","                           num_layers=num_layers, dropout=0.2)\n","\n","    def forward(self, inputs, hidden=None):\n","        embs = self._emb(inputs)\n","        # seq_len, batch_size, 1\n","        _, h = self._rnn(embs, hidden) # у GRU нет h_c, а output нам не нужен\n","        return h[-1].unsqueeze(0) # нам нужно только h последнего слоя"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Un0AOmdqLPp_"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=256, num_layers=1):\n","        super().__init__()\n","\n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.GRU(input_size=emb_dim, hidden_size=rnn_hidden_dim, num_layers=num_layers)\n","        self._out = nn.Linear(rnn_hidden_dim, vocab_size)\n","\n","    def forward(self, inputs, hidden=None):\n","        embs = self._emb(inputs)\n","        outputs, hidden = self._rnn(embs, hidden)\n","        return self._out(outputs), hidden"]},{"cell_type":"markdown","metadata":{"id":"n9nsO1HCmgn3"},"source":["Модель перевода будет просто сперва вызывать Encoder, а потом передавать его скрытое состояние декодеру в качестве начального."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"vLIGjPOiO7X9"},"outputs":[],"source":["class TranslationModel(nn.Module):\n","    def __init__(self, source_vocab_size, target_vocab_size, emb_dim=128, \n","                 rnn_hidden_dim=256, encoder_layers=2, decoder_layers=1):\n","        \n","        super().__init__()\n","        \n","        self.encoder = Encoder(source_vocab_size, emb_dim, rnn_hidden_dim, num_layers=encoder_layers)\n","        self.decoder = Decoder(target_vocab_size, emb_dim, rnn_hidden_dim, num_layers=decoder_layers)\n","        \n","    def forward(self, source_inputs, target_inputs):\n","        encoder_hidden = self.encoder(source_inputs)\n","        \n","        return self.decoder(target_inputs, encoder_hidden)"]},{"cell_type":"markdown","metadata":{},"source":["Потестим, работает ли наша архитектурка. batch[0] - это английский текст, batch[1] - русский перевод."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"5_qVuSL8QJg4"},"outputs":[{"data":{"text/plain":["(torch.Size([6, 128, 31707]), torch.Size([1, 128, 256]))"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["model = TranslationModel(source_vocab_size=len(vocab_transform[SRC_LANGUAGE]), target_vocab_size=len(vocab_transform[TGT_LANGUAGE]),\n","                        encoder_layers=2).to(DEVICE)\n","\n","outs = model(batch[0].to(DEVICE), batch[1].to(DEVICE))\n","outs[0].shape, outs[1].shape"]},{"cell_type":"markdown","metadata":{"id":"pz4Ckgh1mwm4"},"source":["Реализуем простой перевод - жадный. На каждом шаге будем выдавать самый вероятный из предсказываемых токенов:\n","\n","![](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/greedy_dec.jpg)  \n","*From [tensorflow/nmt](https://github.com/tensorflow/nmt)*"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"h9gmcOC9DwiS"},"outputs":[],"source":["def greedy_decode(model, source_text):\n","    model.eval()\n","    with torch.no_grad():\n","        result = [] # список индексов предсказываемых токенов\n","        inputs = text_transform[SRC_LANGUAGE](source_text).to(DEVICE) # затрансформим инпут\n","        encoder_output = model.encoder(inputs)\n","        current_input = torch.LongTensor([[BOS_IDX]]).to(DEVICE) # стартовый символ - начало строки\n","        current_hidden = encoder_output.unsqueeze(0)\n","        # будем считать, что у нас не может быть длиннее 50 токенов\n","        for _ in range(50):\n","            vocab_logits, current_hidden = model.decoder(current_input, current_hidden) \n","            current_input = vocab_logits.argmax(dim=-1)\n","            # но если сгенерился конец строки, то останавливаем\n","            if current_input.squeeze().item() == EOS_IDX:\n","                break \n","\n","            result.append(current_input)\n","            \n","        return ' '.join(vocab_transform[TGT_LANGUAGE].lookup_tokens(result))"]},{"cell_type":"markdown","metadata":{},"source":["Потестим, работает ли наша функция:"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"mM58pAd6FBml"},"outputs":[{"data":{"text/plain":["'профессионал зла расстоянии облаков природу допрашивают разбираю кабеля переселился попрощаться заговорить такой утомлять переживаете словарём серебряные потеряй разглядеть нечиста погуляла содержимое условия результатами учёным терпимее поручили высказал длинная возбужденной прямолинеен обдумай смеёмся врагами прорыв изучали щекотал прятаться плане планы помогают поспорил встаёшь всеобщее заподозрили Сними обсчитала воображению врёт замёрзнуть избит'"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["greedy_decode(model, \"Do you believe?\")"]},{"cell_type":"markdown","metadata":{"id":"0-ha7DI_ngAO"},"source":["Нужно как-то оценивать модель.\n","\n","Обычно для этого используется [BLEU скор](https://en.wikipedia.org/wiki/BLEU) - что-то вроде точности угадывания n-gram из правильного (референсного) перевода.\n","\n","Интерпретируются оценки BLEU следующим образом:\n","\n","|BLEU Score | Interpretation|\n","| --- | --- |\n","|< 10 | Almost useless|\n","|10 - 19 | Hard to get the gist|\n","|20 - 29 | The gist is clear, but has significant grammatical errors|\n","|30 - 40 | Understandable to good translations|\n","|40 - 50 | High quality translations|\n","|50 - 60 | Very high quality, adequate, and fluent translations|\n","|> 60 | Quality often better than human|\n","\n","<img src=\"https://cloud.google.com/translate/automl/docs/images/bleu_score_range.png\" width=600>\n","\n","В nltk есть реализация этой метрики. Напишем функцию для оценки модели:"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"YjYA3eohGlOA"},"outputs":[],"source":["from nltk.translate.bleu_score import corpus_bleu\n","import numpy as np\n","\n","def evaluate_model(model, iterator):\n","    model.eval()\n","    refs, hyps = [], []\n","    \n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            encoder_hidden = model.encoder(batch[0].to(DEVICE))\n","\n","            hidden = encoder_hidden\n","            result = [torch.LongTensor([BOS_IDX]).expand(1, batch[1].shape[1])]\n","\n","            for _ in range(30):\n","                step, hidden = model.decoder(result[-1].to(DEVICE), hidden)\n","                step = step.argmax(-1)\n","                result.append(step.cpu())\n","\n","            targets = batch[1].data.cpu().numpy().T\n","            _, eos_indices = np.where(targets == EOS_IDX)\n","\n","            targets = [target[:eos_ind] for eos_ind, target in zip(eos_indices, targets)]\n","\n","            refs.extend(targets)\n","\n","            result = torch.cat(result)\n","            result = result.data.cpu().numpy().T\n","            _, eos_indices = np.where(result == EOS_IDX)\n","            result = [res[:eos_ind] for eos_ind, res in zip(eos_indices, result)]\n","            hyps.extend(result)\n","            \n","    return corpus_bleu([[ref] for ref in refs], hyps) * 100"]},{"cell_type":"markdown","metadata":{},"source":["Можно убедиться, что функция работает (и что BLEU у нас... ну... блё)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate_model(model, test_iter)"]},{"cell_type":"markdown","metadata":{},"source":["Ну и петлю обучения. "]},{"cell_type":"code","execution_count":68,"metadata":{"id":"_E2JxfRuphch"},"outputs":[],"source":["import math\n","from tqdm import tqdm\n","tqdm.get_lock().locks = []\n","\n","\n","def do_epoch(model, criterion, data_iter, optimizer=None, name=None):\n","    epoch_loss = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = len(data_iter)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, batch in enumerate(data_iter):                \n","                logits, _ = model(batch[0].to(DEVICE), batch[1].to(DEVICE))\n","                \n","                target = torch.cat((batch[1][1:], batch[1].new_ones((1, batch[1].shape[1])))).to(DEVICE)\n","                loss = criterion(logits.view(-1, logits.shape[-1]), target.view(-1))\n","\n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n","                    optimizer.step()\n","\n","                progress_bar.update()\n","                # PPX - Perplexity\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n","                                                                                         math.exp(loss.item())))\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n","                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n","            )\n","            progress_bar.refresh()\n","\n","    return epoch_loss / batches_count\n","\n","\n","def fit(model, criterion, optimizer, train_iter, epochs_count=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss = do_epoch(model, criterion, train_iter, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            val_loss = do_epoch(model, criterion, val_iter, None, name_prefix + '  Val:')\n","            print('\\nVal BLEU = {:.2f}'.format(evaluate_model(model, val_iter)))"]},{"cell_type":"markdown","metadata":{},"source":["Запустим обучение..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5X2kYDU_rCjP"},"outputs":[],"source":["model = TranslationModel(source_vocab_size=len(vocab_transform[SRC_LANGUAGE]), target_vocab_size=len(vocab_transform[TGT_LANGUAGE]), encoder_layers=2).to(DEVICE)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n","\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=20, val_iter=test_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQUu5aDJvBDx"},"outputs":[],"source":["greedy_decode(model, \"Do you believe?\")"]},{"cell_type":"markdown","metadata":{"id":"xL8uOdOCs3nc"},"source":["## Реализация attention'а\n","\n","В общем случае, attention работает так: пусть у нас есть набор скрытых состояний $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m$ - представлений слов из исходного языка, полученных с помощью энкодера. И есть некоторое текущее скрытое состояние $\\mathbf{h}_i$ - скажем, представление, используемое для предсказания слова на нужном нам языке.\n","\n","Тогда с помощью аттеншена мы можем получить взвешенное представление контекста $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m$ - вектор $\\mathbf{c}_i$:\n","$$\n","\\begin{align}\\begin{split}\n","\\mathbf{c}_i &= \\sum\\limits_j a_{ij}\\mathbf{s}_j\\\\\n","\\mathbf{a}_{ij} &= \\text{softmax}(f_{att}(\\mathbf{h}_i, \\mathbf{s}_j))\n","\\end{split}\\end{align}\n","$$\n","\n","$f_{att}$ - функция, которая говорит, насколько хорошо $\\mathbf{h}_i$ и $\\mathbf{s}_j$ подходят друг другу.\n","\n","Самые популярные её варианты:\n","- Additive attention:\n","$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{v}_a{}^\\top \\text{tanh}(\\mathbf{W}_a\\mathbf{h}_i + \\mathbf{W}_b\\mathbf{s}_j)$$\n","- Dot attention:\n","$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{h}_i^\\top \\mathbf{s}_j$$\n","- Multiplicative attention:\n","$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{h}_i^\\top \\mathbf{W}_a \\mathbf{s}_j$$"]},{"cell_type":"code","execution_count":105,"metadata":{"id":"nNUltDmHtPl0"},"outputs":[],"source":["class AdditiveAttention(nn.Module):\n","    def __init__(self, query_size, key_size, hidden_dim):\n","        super().__init__()\n","\n","        # query - decoder state, (1, batch, rnn_hidden_dim)\n","        # value - all encoder states, (seq_len, batch, rnn_hidden_dim)\n","        # key_proj - self.key_layer(value), (seq_len, batch, hidden_dim)\n","        # hidden_dim - size of attention layer\n","        \n","        self._query_layer = nn.Linear(query_size, hidden_dim)\n","        self._key_layer = nn.Linear(key_size, hidden_dim)\n","        self._energy_layer = nn.Linear(hidden_dim, 1)\n","        \n","    def forward(self, query, value, mask=None):\n","        # получаем Key\n","        key_proj = self._key_layer(value)\n","        f_att = self._energy_layer(torch.tanh(self._query_layer(query) + key_proj))\n","        f_att = F.softmax(f_att, 0)\n","        scores = f_att * value \n","        return scores.sum(0), f_att"]},{"cell_type":"markdown","metadata":{"id":"tWiXuWHltVpU"},"source":["Обновим декодер и энкодер"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=256, num_layers=1):\n","        super().__init__()\n","        \n","        self._num_layers = num_layers\n","        self._hidden_dim = rnn_hidden_dim\n","        \n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.GRU(input_size=emb_dim, hidden_size=self._hidden_dim, \n","                           num_layers=num_layers, dropout=0.2)\n","\n","    def forward(self, inputs, hidden=None):\n","        embs = self._emb(inputs)\n","        # seq_len, batch_size, 1\n","        o, h = self._rnn(embs, hidden) \n","        return o, h[-1].unsqueeze(0) # будем аутпуты тоже брать - они нужны для аттеншна"]},{"cell_type":"code","execution_count":125,"metadata":{"id":"NaAX28iQtVAs"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, vocab_size, emb_dim=64, rnn_hidden_dim=256, attn_dim=128, num_layers=1):\n","        super().__init__()\n","\n","        self._emb = nn.Embedding(vocab_size, emb_dim)\n","        self._rnn = nn.GRU(input_size=emb_dim + rnn_hidden_dim, \n","                           hidden_size=rnn_hidden_dim, num_layers=num_layers)\n","        self._out = nn.Linear(rnn_hidden_dim, vocab_size) \n","        self._att = AdditiveAttention(rnn_hidden_dim, rnn_hidden_dim, attn_dim) # добавляем слой с аттеншном\n","        self._drop = nn.Dropout(p=0.3)\n","\n","    def forward(self, inputs, encoder_output, encoder_mask, hidden=None):\n","        embs = self._emb(inputs)\n","        outputs = []\n","        attentions = []\n","        for i in range(embs.shape[0]):\n","            context, f_att = self._att(query=hidden, value=encoder_output, mask=encoder_mask)\n","            context = context.unsqueeze(0)\n","            rnn_input = torch.cat((embs[i:i + 1], context), -1)\n","            output, hidden = self._rnn(rnn_input, hidden)\n","\n","            outputs.append(output)\n","            attentions.append(f_att)\n","\n","        output = self._drop(torch.cat(outputs))\n","        attentions = torch.cat(attentions)\n","        return self._out(output), hidden, attentions"]},{"cell_type":"markdown","metadata":{},"source":["Ну и модельку - чуточку. Теперь у нас есть аттеншн и его мерность"]},{"cell_type":"code","execution_count":126,"metadata":{"id":"S9L_aBVDs39U"},"outputs":[],"source":["class TranslationModel(nn.Module):\n","    def __init__(self, source_vocab_size, target_vocab_size, emb_dim=64, rnn_hidden_dim=128, \n","                 attn_dim=128, encoder_num_layers=2):\n","        \n","        super().__init__()\n","        \n","        self.encoder = Encoder(source_vocab_size, emb_dim, rnn_hidden_dim, encoder_num_layers)\n","        self.decoder = Decoder(target_vocab_size, emb_dim, rnn_hidden_dim, attn_dim, 1)\n","        \n","    def forward(self, source_inputs, target_inputs):\n","        encoder_mask = source_inputs == 1\n","        encoder_output, encoder_hidden = self.encoder(source_inputs)\n","        return self.decoder(target_inputs, encoder_output, encoder_mask, encoder_hidden)"]},{"cell_type":"markdown","metadata":{},"source":["Проверим, что архитектурку не поломали"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZmMUAMktB3s"},"outputs":[],"source":["model = TranslationModel(source_vocab_size=len(vocab_transform[SRC_LANGUAGE]), target_vocab_size=len(vocab_transform[TGT_LANGUAGE])).to(DEVICE)\n","\n","model(batch[0].to(DEVICE), batch[1].to(DEVICE))"]},{"cell_type":"markdown","metadata":{},"source":["Понадобится немного переписать функцию оценки и петлю:"]},{"cell_type":"code","execution_count":162,"metadata":{"id":"yQH0KwBsth1A"},"outputs":[],"source":["def evaluate_model(model, iterator):\n","    model.eval()\n","    refs, hyps = [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            encoder_output, encoder_hidden = model.encoder(batch[0].to(DEVICE))\n","            mask = batch[0] == 1.\n","            \n","            hidden = encoder_hidden\n","            result = [torch.LongTensor([BOS_IDX]).expand(1, batch[0].shape[1])]\n","            \n","            for _ in range(30):\n","                step, hidden, _ = model.decoder(result[-1].to(DEVICE), encoder_output, mask, hidden)\n","                step = step.argmax(-1)\n","                result.append(step.cpu())\n","            \n","            targets = batch[1].data.cpu().numpy().T\n","            eos_indices = (targets == EOS_IDX).argmax(-1)\n","            eos_indices[eos_indices == 0] = targets.shape[1]\n","\n","            targets = [target[:eos_ind] for eos_ind, target in zip(eos_indices, targets)]\n","            refs.extend(targets)\n","            \n","            result = torch.cat(result)\n","            result = result.data.cpu().numpy().T\n","            eos_indices = (result == EOS_IDX).argmax(-1)\n","            eos_indices[eos_indices == 0] = result.shape[1]\n","\n","            result = [res[:eos_ind] for eos_ind, res in zip(eos_indices, result)]\n","            hyps.extend(result)\n","            \n","    return corpus_bleu([[ref] for ref in refs], hyps) * 100\n","\n","def do_epoch(model, criterion, data_iter, optimizer=None, name=None):\n","    epoch_loss = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = len(data_iter)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, batch in enumerate(data_iter):                \n","                # выхлоп стал больше на один\n","                logits, _, _ = model(batch[0].to(DEVICE), batch[1].to(DEVICE))\n","                \n","                target = torch.cat((batch[1][1:], batch[1].new_ones((1, batch[1].shape[1])))).to(DEVICE)\n","                loss = criterion(logits.view(-1, logits.shape[-1]), target.view(-1))\n","\n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n","                    optimizer.step()\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n","                                                                                         math.exp(loss.item())))\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n","                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n","            )\n","            progress_bar.refresh()\n","\n","    return epoch_loss / batches_count\n","\n","\n","def fit(model, criterion, optimizer, train_iter, epochs_count=1, val_iter=None):\n","    best_val_loss = None\n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss = do_epoch(model, criterion, train_iter, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_iter is None:\n","            val_loss = do_epoch(model, criterion, val_iter, None, name_prefix + '  Val:')\n","            print('\\nVal BLEU = {:.2f}'.format(evaluate_model(model, val_iter)))"]},{"cell_type":"markdown","metadata":{},"source":["Потестим, что оценка модели работает"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluate_model(model, test_iter)"]},{"cell_type":"markdown","metadata":{},"source":["Ну и запустим обучение. На этот раз нам должно понадобиться меньше эпох, чтобы достичь приличного качества"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xCA3DhBtvYM"},"outputs":[],"source":["model = TranslationModel(source_vocab_size=len(vocab_transform[SRC_LANGUAGE]), target_vocab_size=len(vocab_transform[TGT_LANGUAGE])).to(DEVICE)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n","\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","fit(model, criterion, optimizer, train_iter, epochs_count=9, val_iter=test_iter)"]},{"cell_type":"markdown","metadata":{},"source":["Придется переписать и функцию перевода. Заодно давайте отрисуем красивенький график с аттеншном: для этого пусть наша функция возвращает токены исходного предложения, токены результата и аттеншн"]},{"cell_type":"code","execution_count":193,"metadata":{"id":"kXOkzWyIuDRo"},"outputs":[],"source":["def greedy_decode(model, source_text):  \n","    model.eval()\n","    with torch.no_grad():\n","        result, attentions = [], []\n","        source = token_transform[SRC_LANGUAGE](source_text)\n","        inputs = text_transform[SRC_LANGUAGE](source_text).to(DEVICE)\n","        \n","        encoder_output, encoder_hidden = model.encoder(inputs)\n","        encoder_mask = torch.zeros_like(inputs).byte()\n","        \n","        hidden = encoder_hidden\n","        step = torch.LongTensor([BOS_IDX]).to(DEVICE)\n","        \n","        for _ in range(50):\n","            step, hidden, attention = model.decoder(step, encoder_output, encoder_mask, hidden)\n","            step = step.argmax(-1)\n","            attentions.append(attention)\n","          \n","            if step.squeeze().item() == EOS_IDX:\n","                break\n","            \n","            result.append(step.item())   \n","        result = vocab_transform[TGT_LANGUAGE].lookup_tokens(result)\n","        return source, result, torch.cat(attentions, -1).data.cpu().numpy()"]},{"cell_type":"code","execution_count":185,"metadata":{"id":"0dacXWf3uETQ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","def plot_heatmap(src, trg, scores):\n","\n","    fig, ax = plt.subplots()\n","    heatmap = ax.pcolor(scores, cmap='viridis')\n","\n","    ax.set_xticklabels(trg, minor=False, rotation=45)\n","    ax.set_yticklabels(src, minor=False)\n","\n","    ax.xaxis.tick_top()\n","    ax.set_xticks(np.arange(scores.shape[1]) + 0.5, minor=False)\n","    ax.set_yticks(np.arange(scores.shape[0]) + 0.5, minor=False)\n","    ax.invert_yaxis()\n","\n","    plt.colorbar(heatmap)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Готово, вы великолепны (но модель нет)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAnRu2cMuFno"},"outputs":[],"source":["source, result, attentions = greedy_decode(model, \"I didn't pay.\")\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cJcUv_suGns"},"outputs":[],"source":["plot_heatmap(['<s>'] + source, result + ['</s>'], attentions)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Seq2seq","provenance":[{"file_id":"1mwhfj4KYLlZPpQUwF01zGPpdowrnBIcU","timestamp":1648117106000},{"file_id":"11RMkt5nXysqniaAa-gBZ_scQbNc1MeSM","timestamp":1542738214842},{"file_id":"1jSYWuEGwik2lnnvGSU_PyXTFtbRKSyz_","timestamp":1541488893289},{"file_id":"1W5uaNpKFoaq1gV9N9FpIAEDyrsGGRBBi","timestamp":1540140192448}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
